---
title: "PUBG Top 10% Placement Analysis"
author: "Chance Robinson, Allison Roderick and William Arnost"
date: |
  Master of Science in Data Science, Southern Methodist University, USA
lang: en-US
class: man
numbersections: true
encoding: UTF-8
bibliography: references.bib
biblio-style: apalike
output:
  bookdown::pdf_document2:
     citation_package: natbib
     keep_tex: true
     toc: false
header-includes:
   - \usepackage{amsmath}
   - \usepackage[utf8]{inputenc}
   - \usepackage[T1]{fontenc}
   - \usepackage{setspace}
   - \usepackage{hyperref}
   - \onehalfspacing
   - \setcitestyle{numbers,square,super}
   - \newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
editor_options:
  chunk_output_type: console



---
```{r, lib-read, results='hide', message=FALSE, include=FALSE, echo=FALSE, eval=TRUE}

#setwd('./src/')

# library imports
library(MASS)
library(tidyverse)
library(knitr)

# Random Forest
library(randomForest)  
# downSample
library(caret)
# ROC Curves
library(ROCR)
library(pROC)
# outlier and leverage plots
library(olsrr)
# LDA
library(dplyr)
library(car)
library(RColorBrewer)
library(psych)
library(glmnet)
library(reshape2)

library(e1071)
library(corrplot)
library(GGally)
library(formattable)
library(dlookr)
```


```{r, load-data, results='hide', message=FALSE, include=FALSE, echo=FALSE, eval=TRUE}

train <- read.csv("../data/pubg_solo_game_types_train_downsampled.csv", stringsAsFactors=FALSE)
test <- read.csv("../data/pubg_solo_game_types_test_full.csv", stringsAsFactors=FALSE)
solo <- read_csv("../data/pubg_solo_game_types.csv") %>% dplyr::select(-c("DBNOs","revives"))

# Chance
# train <- read.csv("C:/Users/Chance/github/smu/PUBGFinishPlacementAnalysis/data/datapubg_solo_game_types_train_downsampled.csv", stringsAsFactors=FALSE)
# test <- read.csv("C:/Users/Chance/github/smu/PUBGFinishPlacementAnalysis/data/pubg_solo_game_types_test_full.csv", stringsAsFactors=FALSE)

# Allison
# train <- read.csv("C:/Git/PUBGFinishPlacementAnalysis/data/pubg_solo_game_types_train_downsampled.csv", stringsAsFactors=FALSE)
# test <- read.csv("C:/Git/PUBGFinishPlacementAnalysis/data/pubg_solo_game_types_test_full.csv", stringsAsFactors=FALSE)

train <- train %>%
  mutate(top.10 = factor(top.10, labels = c("0", "1")))


test <- test %>%
  mutate(top.10 = factor(top.10, labels = c("0", "1")))


```


# Introduction

[Intro]


# Data Description

The source data is available on Kaggle.com under the competition PUBG Finish Placement Prediction. The files used in our analysis were transformed to fit the requirements of a binomial logistic regression classifier.  The data has also been pre-split into training and test files for consistency when comparing our different model types.  Additionally, as the percentage-based nature of the top 10% of players is inherently unbalanced, we've also downsampled the higher frequency data to match that over the lower frequency outcome of the top 10% of players.


`pubg_solo_game_types.csv`  

* Filtered for solo only game types

`pubg_solo_game_types_test_full.csv`  

* Pre-split for test data

`pubg_solo_game_types_train_full.csv`  

* Pre-split for train data without downsampling for the unbalanced response variable

`pubg_solo_game_types_train_downsampled.csv`  

* Pre-split for train data with downsampling for the unbalanced response variable




## Data Dictionary

Column Name      |Type           |Description
-----------------|---------------|-------------------------------------------------
DBNOs            |               |Number of enemy players knocked.
assists          |               |Number of enemy players this player damaged that were killed by teammates.
boosts           |               |Number of boost items used.
damageDealt      |               |Total damage dealt. Note: Self inflicted damage is subtracted.
headshotKills    |               |Number of enemy players killed with headshots.
heals            |               |Number of healing items used.
Id               |               |Players Id
killPlace        |               |Ranking in match of number of enemy players killed.
killPoints       |               |Kills-based external ranking of player. (Think of this as an Elo ranking where only kills matter.) If there is a value other than -1 in rankPoints, then any 0 in killPoints should be treated as a None.
killStreaks      |               |Max number of enemy players killed in a short amount of time.
kills            |               |Number of enemy players killed.
longestKill      |               |Longest distance between player and player killed at time of death. 
matchDuration    |               |Duration of match in seconds.
matchId          |               |ID to identify match. There are no matches that are in both the training and testing set.
matchType        |               |String identifying the game mode that the data comes from. 
rankPoints       |               |Elo-like ranking of player. 
revives          |               |Number of times this player revived teammates.
rideDistance     |               |Total distance traveled in vehicles measured in meters.
roadKills        |               |Number of kills while in a vehicle.
swimDistance     |               |Total distance traveled by swimming measured in meters.
teamKills        |               |Number of times this player killed a teammate.
vehicleDestroys  |               |Number of vehicles destroyed.
walkDistance     |               |Total distance traveled on foot measured in meters.
weaponsAcquired  |               |Number of weapons picked up.
winPoints        |               |Win-based external ranking of player. (Think of this as an Elo ranking where only winning matters.) If there is a value other than -1 in rankPoints, then any 0 in winPoints should be treated as a None.
groupId          |               |ID to identify a group within a match. If the same group of players plays in different matches, they will have a different groupId each time.
numGroups        |               |Number of groups we have data for in the match.
maxPlace         |               |Worst placement we have data for in the match. This may not match with numGroups, as sometimes the data skips over placements.
winPlacePerc     |               |This is a percentile winning placement, where 1 corresponds to 1st place, and 0 corresponds to last place in the match. (to be removed from our binomial classfier so as not to influence our predictive results)
top.10           |               |The target of prediction. This is a percentile winning placement, where 1 corresponds to a top 10% placement a 0 in the lower 90%.


\newpage


## Exploratory Data Analysis


# Objective I Analysis  

```{r Obj1_Data_Prep, results='hide', message=FALSE, include=FALSE, echo=FALSE, eval=TRUE}
library(tidyverse)
solo <- solo %>% mutate(
DurationCut = as.factor(ifelse(matchDuration > 1600,1,0)),
kpCut = as.factor(ifelse(killPoints > 0,1,0)),
wpCut = as.factor(ifelse(winPoints > 0,1,0)),
rpCut = as.factor(ifelse(rankPoints > -1,1,0)),
boostCut = as.factor(ifelse(boosts >= 2,1,0)),
healCut = as.factor(ifelse(heals >= 2,1,0)),
walkCut = as.factor(ifelse(walkDistance >= 1250,1,0)),
weaponCut = as.factor(ifelse(weaponsAcquired >= 3,1,0)),
killCut = as.factor(ifelse(kills >= 2,1,0)),
assistCut = as.factor(ifelse(assists >= 2,1,0)),
damageCut = as.factor(ifelse(damageDealt >= 250,1,0)),
streakCut = as.factor(ifelse(killStreaks >= 1,1,0)),
killsPK = kills/(walkDistance+1/1000),
damageKill = kills/(damageDealt+1)
)

test_obj1 <- test %>% mutate(
  DurationCut = as.factor(ifelse(matchDuration > 1600,1,0)),
  kpCut = as.factor(ifelse(killPoints > 0,1,0)),
  wpCut = as.factor(ifelse(winPoints > 0,1,0)),
  rpCut = as.factor(ifelse(rankPoints > -1,1,0)),
  boostCut = as.factor(ifelse(boosts >= 2,1,0)),
  healCut = as.factor(ifelse(heals >= 2,1,0)),
  walkCut = as.factor(ifelse(walkDistance >= 1250,1,0)),
  weaponCut = as.factor(ifelse(weaponsAcquired >= 3,1,0)),
  killCut = as.factor(ifelse(kills >= 2,1,0)),
  assistCut = as.factor(ifelse(assists >= 2,1,0)),
  damageCut = as.factor(ifelse(damageDealt >= 250,1,0)),
  streakCut = as.factor(ifelse(killStreaks >= 1,1,0)),
  killsPK = kills/(walkDistance+1/1000),
  damageKill = kills/(damageDealt+1)
)

train_obj1 <- train %>% mutate(
  DurationCut = as.factor(ifelse(matchDuration > 1600,1,0)),
  kpCut = as.factor(ifelse(killPoints > 0,1,0)),
  wpCut = as.factor(ifelse(winPoints > 0,1,0)),
  rpCut = as.factor(ifelse(rankPoints > -1,1,0)),
  boostCut = as.factor(ifelse(boosts >= 2,1,0)),
  healCut = as.factor(ifelse(heals >= 2,1,0)),
  walkCut = as.factor(ifelse(walkDistance >= 1250,1,0)),
  weaponCut = as.factor(ifelse(weaponsAcquired >= 3,1,0)),
  killCut = as.factor(ifelse(kills >= 2,1,0)),
  assistCut = as.factor(ifelse(assists >= 2,1,0)),
  damageCut = as.factor(ifelse(damageDealt >= 250,1,0)),
  streakCut = as.factor(ifelse(killStreaks >= 1,1,0)),
  killsPK = kills/(walkDistance+1/1000),
  damageKill = kills/(damageDealt+1)
)

## References to groups of columns
varGroupKill <- c("damageDealt","headshotKills","kills","killStreaks","roadKills","vehicleDestroys","weaponsAcquired")
varGroupTeam <- c("assists","boosts","heals","teamKills")
varGroupDistance <- c("longestKill","rideDistance","swimDistance","walkDistance")
varGroupPoints <- c("killPoints","winPoints","rankPoints","winPlacePerc","matchDuration")

## Helper Functions
plotRoc <- function(preds, truth) {
  pred <- prediction(preds, truth)
  roc.perf = performance(pred, measure = "tpr", x.measure = "fpr")
  auc.train <- performance(pred, measure = "auc")
  auc.train <- auc.train@y.values
  
  #Plot ROC
  par(mar=c(4,4,4,4))
  plot(roc.perf,main="Ordinary Logistic")
  abline(a=0, b= 1) #Ref line indicating poor performance
  text(x = .40, y = .6,paste("AUC = ", round(auc.train[[1]],3), sep = ""))
  table(test$top.10, useNA = "ifany")
}
scoreMod <- function(model.out) {
  model.pred <- predict(model.out, newdata = test, type = "response")
  plotRoc(model.pred,test_obj1$top.10)
  confusionMatrix(as.factor(as.numeric(model.pred > 0.5)), test_obj1$top.10, positive = "1")
}
```


## Problem Statement  

We would like to predict if a player finished in the top 10 of a PUBG match based on their stats during the match. Our response variable is top.10, which is 1 if the player finished in the top 10 and 0 if they did not. We will start with a logistic regression model before moving into more complex methods.  

## Establishing the Initial Model  

During EDA, we discovered a number of variables that would not be useful for prediction, either because they were ID columns or match characteristics that were not relevant to individual player performance. Removing those, our first model contains all the remaining variables.  
```{r obj1_model1, message=FALSE, include=TRUE, echo=FALSE, eval=TRUE}
## Logistic Model 1 - "The Kitchen Sink"
model.main <- glm(top.10~assists + boosts + heals + teamKills + weaponsAcquired +
                    damageDealt + headshotKills + kills + killStreaks + roadKills + vehicleDestroys + 
                    killPlace + killPoints  + rankPoints + winPoints +
                    longestKill + swimDistance + rideDistance + walkDistance,
                  data= train_obj1, family = binomial(link="logit"))
summary(model.main)
#vif(model.main) ## Kill points and win points super correlated
#scoreMod(model.main)
```

We can see some insignificant variables, and the vif (Appendix Ref) will show some correlation. This model performance of this model is as follows: 

Model   |AUC     |Sensitivity |Specificity | Error Rate (1 - Accuracy)
--------|--------|------------|------------|--------------------------
Initial | 0.941  | 0.891      | 0.843      | 15.2%


## Variable Selection 

Next, we use lasso to narrow these down to the most important variables. First, we used cv.glmnet to search for the ideal value of lambda for the lasso regression (Appendix Reference). We selected the lambda.1se value, which will hopefully eliminate the most variables. Using that value for lambda, lasso produced this output:

```{r obj1_lasso, message=FALSE, include=TRUE, echo=FALSE, eval=TRUE}
set.seed(1234)
x <- model.matrix(top.10~assists + boosts + heals + teamKills + weaponsAcquired +
                    damageDealt + headshotKills + kills + killStreaks + roadKills + vehicleDestroys + 
                    killPlace + killPoints  + rankPoints + winPoints +
                    longestKill + swimDistance + rideDistance + walkDistance, data=train_obj1)
cv.lasso <- cv.glmnet(x, train_obj1$top.10, alpha = 1, family = "binomial")
plot(cv.lasso)

#cv.lasso$lambda.min
#coef(cv.lasso, cv.lasso$lambda.min)
#cv.lasso$lambda.1se
#coef(cv.lasso, cv.lasso$lambda.1se)


model.lasso <- glmnet(x, train_obj1$top.10, alpha = 1, family = "binomial",
                      lambda = cv.lasso$lambda.1se)

coef(model.lasso)

#x.test <- model.matrix(top.10~assists + boosts + heals + teamKills + weaponsAcquired +
#                    damageDealt + headshotKills + kills + killStreaks + roadKills + vehicleDestroys + 
#                    killPlace + killPoints  + rankPoints + winPoints +
#                    longestKill + swimDistance + rideDistance + walkDistance, data=test_obj1)

#lasso.pred <- predict(model.lasso, newx = x.test)
#plotRoc(lasso.pred,test_obj1$top.10)
#confusionMatrix(as.factor(as.numeric(lasso.pred > 0.5)), test_obj1$top.10, positive = "1")
```

## Final Iterations

The selection eliminated damageDealt, headshotKills, and killPoints. We created a model using just the variables from lasso (ref) and found two issues. First, winPoints and rankPoints have high VIFs. Removing them individually found that when one is missing, the other is no longer significant. We decided to remove them both. Second, the cooksD plot shows 3 potentially influential points. All three points have high values ( > p99 threshold) for roadKills and killStreaks, and I also found through adding and removing variables that swimDistance contributes influential points. We chose to remove these variables, as a very small part of the population has high values in these metrics, and it negatively effects the model.  

```{r obj1_final, message=FALSE, include=TRUE, echo=FALSE, eval=TRUE}
model.final.lasso <- glm(top.10~assists + boosts + heals + teamKills + weaponsAcquired +
                    kills + killStreaks + roadKills + vehicleDestroys + 
                    killPlace + rankPoints + winPoints +
                    longestKill + swimDistance + rideDistance + walkDistance,
                  data= train_obj1, family = binomial(link="logit"))
#summary(model.final.lasso)
#vif(model.final.lasso) ## Kill points and win points super correlated
```

Here we can see influential points from the Cook's D chart.  

```{r obj1_finalb, message=FALSE, include=TRUE, echo=FALSE, eval=TRUE}
plot(model.final.lasso, which = 4, id.n = 3)
#scoreMod(model.final.lasso)


#model.data %>% top_n(3, .cooksd)

## trying to remove some of the influential points and correlated vars
  #No rankpoints

model.inf1 <- glm(top.10~assists + boosts + heals + teamKills + weaponsAcquired +
                           kills + killStreaks + roadKills + vehicleDestroys + 
                           killPlace + winPoints +
                           longestKill + swimDistance + rideDistance + walkDistance,
                         data= train_obj1, family = binomial(link="logit"))
#summary(model.inf1)
#vif(model.inf1)
##winpoints no longer significant  
#No winpoints
model.inf2 <- glm(top.10~assists + boosts + heals + teamKills + weaponsAcquired +
                    kills + killStreaks + roadKills + vehicleDestroys + 
                    killPlace + rankPoints +
                    longestKill + swimDistance + rideDistance + walkDistance,
                  data= train_obj1, family = binomial(link="logit"))
#summary(model.inf2)
#  vif(model.inf2)
##rankpoints not significant

##neither

model.inf2a <- glm(top.10~assists + boosts + heals + teamKills + weaponsAcquired +
                    kills + vehicleDestroys + killPlace + killStreaks + roadKills + swimDistance +
                    longestKill + rideDistance + walkDistance,
                  data= train_obj1, family = binomial(link="logit"))
#summary(model.inf2a)
#vif(model.inf2a)

#plot(model.inf2a, which = 4, id.n = 3)

## removing killStreaks + roadKills + swimDistance
model.inf3 <- glm(top.10~assists + boosts + heals + teamKills + weaponsAcquired +
                  kills + vehicleDestroys + killPlace + 
                  longestKill + rideDistance + walkDistance,
                data= train_obj1, family = binomial(link="logit"))
#summary(model.inf3)
#vif(model.inf3)
```

\newpage

After removing roadKills , killStreaks, and swimDistance  

```{r obj1_finalc, message=FALSE, include=TRUE, echo=FALSE, eval=TRUE}
plot(model.inf3, which = 4, id.n = 3)

#scoreMod(model.inf3)
```

Final Model Coefficients  

```{r obj1_finald, message=FALSE, include=TRUE, echo=FALSE, eval=TRUE}
#assists + boosts + heals + weaponsAcquired + kills + walkDistance
model.inf4 <- glm(top.10~assists + boosts + heals + weaponsAcquired + killPlace + walkDistance ,
                  data= train_obj1, family = binomial(link="logit"))
summary(model.inf4)
#vif(model.inf4)
#plot(model.inf4, which = 4, id.n = 3)

#scoreMod(model.inf4)
```

## Comparing Competing Models

Model   |AUC     |Sensitivity |Specificity | Error Rate (1 - Accuracy)
--------|--------|------------|------------|--------------------------
Initial | 0.941  | 0.891      | 0.843      | 15.2%
Lasso   | 0.941  | 0.892      | 0.843      | 15.2%
Inf. Points   | 0.936  | 0.892      | 0.842      | 15.3%
Final   | 0.932  | 0.863      | 0.841      | 15.6%


## Model Interpretation

We prefer the final model because it simplifies the previous versions without significant loss in accuracy.  The variable interpretations are found in the table below.

Variable        | Interpretation
----------------|--------------------------
assists         | The log-odds of placing in the top 10 increase by 0.6225 for each additional assist
boosts          | The log-odds of placing in the top 10 increase by 0.3067 for each additional boost
heals           | The log-odds of placing in the top 10 decrease by -0.0228 for each additional heal
weaponsAcquired | The log-odds of placing in the top 10 increase by 0.0790 for each additional weapon acquired
killPlace       | The log-odds of placing in the top 10 increase by 0.0622 for each additional rank in kills (lower is better for killPlace)
walkDistance    | The log-odds of placing in the top 10 increase by 0.0007 for each additional meter walked


## Conclusion

Players who do well in kill placement and who travel far tend to win games. Travel distance probably is a proxy for time alive, as players eliminated early would probably not travel far. Acquiring in game items like boosts or weapons also help. The negative coeffient on heals is odd, perhaps indicating that taking more damage is detrimental to placement.  We will try to improve on the final model using more complex methods.

\newpage


# Objective II Analysis


## Question of Interest


## Model Selection


### Linear Discriminant Analysis
Our next prediction tool is Linear Discriminant Analysis (LDA) for classifying the player as Top 10 or not. We have taken a subset of the continuous variables from our EDA to build the LDA off of. Before running the LDA, we will cover two things: a LASSO call to eliminate less important variables and assumption checking.

#### LASSO

The LASSO call plus manual variable selection reduced the predictors considered for the LDA model to: boosts, heals, killPlace, killStreaks, longestKill, matchDuration, rideDistance, swimDistance, teamKills, walkDistance, weaponsAcquired.

```{r lda.datapull, results='hide', message=FALSE, include=FALSE, echo=FALSE, eval=TRUE}
# Reduce variables to relevant continuous variables
train1<-train[,c(5:6,8:10,12:15,19,21:27,29:30)]
test1<-test[,c(5:6,8:10,12:15,19,21:27,29:30)]
# train2 <- train1[,c(1,4:19)]
# test2 <- test1[,c(1,4:19)]
train2_alt <- train1[,c(1,4:17,19)]
test2_alt <- test1[,c(1,4:17,19)]
```

```{r lda.lasso, results='hide', message=FALSE, include=FALSE, echo=FALSE, eval=TRUE}

# Get data in format for LASSO
x=model.matrix(top.10~.,train2_alt)[,-1]
y=as.numeric(train2_alt$top.10)

xtest<-model.matrix(top.10~.,test2_alt)[,-1]
ytest<-as.numeric(test2_alt$top.10)

grid=10^seq(10,-2, length =100)
lasso.mod=glmnet(x,y,alpha=1, lambda =grid)

set.seed(23) #removes kills roadKills vehicleDestroys
cv.out=cv.glmnet(x,y,alpha=1,family="binomial") #alpha=1 performs LASSO
lda.lasso<-plot(cv.out)

# simplest model
lda.lasso.model.coef<-coef(cv.out, cv.out$lambda.1se)
lda.lasso.model.coef
```

```{r lda.train_final, results='hide', message=FALSE, include=FALSE, echo=FALSE, eval=TRUE}

# Removing the kills, roadKills, and vehicleDestroys as shown above
# Also removing rankPoints because 
# (a) the majority of the players in these data aren't ranked and 
# (b) the Kaggle descrption says "This ranking is inconsistent and is being deprecated in the API's next version"
train_final <- train2_alt[,c(-4,-8,-10,-13)]

```
#### Assumption Checking

LDA performs optimally when the assumptions of MANOVA are met. That is, 

1. The predictors are normally distributed for each class of the response.

2. The covariance matrices for each class of the response are homogeneous.

When we check the first assumption for the predictors that are to be included in the LDA model, we see that the assumption is not met. Most of the predictors are right skewed. The variable matchDuration is bimodal. To remedy this, we tried transforming the variables, but it did not help our overall prediction accuracy. However, because issues of normality exist, we will explore QDA as well as LDA.

```{r lda.assumptions.1, results='hide', message=FALSE, include=FALSE, echo=FALSE, eval=TRUE}

train_final_no <- train_final[which(train_final$top.10==0),]
train_final_yes <- train_final[which(train_final$top.10==1),]

# nrow(train_final)
# nrow(train_final_no)
# nrow(train_final_yes)

max_cols<-ncol(train_final)
no_matrix<-as.matrix(train_final_no[,-max_cols])
yes_matrix<-as.matrix(train_final_yes[,-max_cols])

no_hist<-multi.hist(no_matrix)
yes_hist<-multi.hist(yes_matrix)
# par(mfrow=c(1,1))

```

We also checked the homogeneity of correlation matrices. we find that, overall, there are no major departures from homogeneity. The variables walkDistance and killPlace show the greatest deviances from homogenous correlations between bottom 90 and top 10 placements. Consequently, we tried removing those variables from the model. However, removing those variables reduced our prediction accuracy.

Thus, we will proceed with the variables selected and see if LDA or QDA performs better.

```{r lda.assumptions.2, results='hide', message=FALSE, include=FALSE, echo=FALSE, eval=TRUE}

#http://www.sthda.com/english/wiki/ggplot2-quick-correlation-matrix-heatmap-r-software-and-data-visualization
get_upper_tri <- function(cormat){
  cormat[lower.tri(cormat)]<- NA
  return(cormat)
}

custom_corr_plot <- function(cormat){
  
  upper_tri <- get_upper_tri(cormat)
  # Melt the correlation matrix
  melted_cormat <- melt(upper_tri, na.rm = TRUE)
  # Create a ggheatmap
  ggheatmap <- ggplot(melted_cormat, aes(Var2, Var1, fill = value))+
    geom_tile(color = "white")+
    scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                         midpoint = 0, limit = c(-1,1), space = "Lab", 
                         name="Pearson\nCorrelation") +
    theme_minimal()+ # minimal theme
    theme(axis.text.x = element_text(angle = 45, vjust = 1, 
                                     size = 12, hjust = 1))+
    coord_fixed()
  
  
  p<-ggheatmap + 
    geom_text(aes(Var2, Var1, label = value), color = "black", size = 4) +
    theme(
      axis.title.x = element_blank(),
      axis.title.y = element_blank(),
      panel.grid.major = element_blank(),
      panel.border = element_blank(),
      panel.background = element_blank(),
      axis.ticks = element_blank(),
      legend.justification = c(1, 0),
      legend.position = c(0.6, 0.7),
      legend.direction = "horizontal")+
    guides(fill = guide_colorbar(barwidth = 7, barheight = 1,
                                 title.position = "top", title.hjust = 0.5))
  
  return(p)
}

cormat <- round(cor(no_matrix),2)
no <- custom_corr_plot(cormat)

cormat <- round(cor(yes_matrix),2)
yes <- custom_corr_plot(cormat)

```

#### LDA Results

LDA has a prediction accuracy of 0.9206, with a sensitivity of 0.57479 and a specificity of 0.96099. The area under the ROC curve is 0.941.

```{r lda.model, results='hide', message=FALSE, include=FALSE, echo=FALSE, eval=TRUE}

# Run LDA
lda <- lda(top.10 ~ . , data=train_final, prior=c(.9,.1))

lda

predict <- predict(lda,newdata=test)

test_final <- test
test_final$predcited_place <- as.vector(predict$class)
test_final$top.10<-as.factor(test_final$top.10)
test_final$predcited_place<-as.factor(test_final$predcited_place)

xtab<-table(test_final$predcited_place,test_final$top.10)
confusionMatrix(xtab, positive="1")

predict.posteriors <- as.data.frame(predict$posterior)

# Evaluate the model
pred <- prediction(predict.posteriors[,2], test$top.10)
roc.perf = performance(pred, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred, measure = "auc")
auc.train <- auc.train@y.values


# Plot
plot(roc.perf, main="ROC Curve - LDA")
abline(a=0, b= 1)
text(x = .25, y = .65 ,paste("AUC = ", round(auc.train[[1]],3), sep = ""))


```

#### QDA Results

QDA has a prediction accuracy of 0.8779, with a sensitivity of 0.68262 and a specificity of 0.90071. The area under the ROC curve is 0.912.

```{r qda.model, results='hide', message=FALSE, include=FALSE, echo=FALSE, eval=TRUE}

qda <- qda(top.10 ~ . , data=train_final, prior=c(.9,.1))

qda

predict <- predict(qda,newdata=test)

test_final <- test
test_final$predcited_place <- as.vector(predict$class)
test_final$top.10<-as.factor(test_final$top.10)
test_final$predcited_place<-as.factor(test_final$predcited_place)
# test_final$predcited_place<-as.factor(test_final$predcited_place)
str(test_final)

xtab<-table(test_final$predcited_place,test_final$top.10)
confusionMatrix(xtab, positive="1")
# confusionMatrix(test_final$predcited_place, test_final$top.10)

predict.posteriors <- as.data.frame(predict$posterior)

# Evaluate the model
pred <- prediction(predict.posteriors[,2], test$top.10)
roc.perf = performance(pred, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred, measure = "auc")
auc.train <- auc.train@y.values


# Plot
plot(roc.perf, main="ROC Curve - QDA")
abline(a=0, b= 1)
text(x = .25, y = .65 ,paste("AUC = ", round(auc.train[[1]],3), sep = ""))


```

#### LDA Conclusion
Although the QDA is better at predicting the top 10 placements that were true top 10 than LDA (QDA sensitivity of 0.68 > LDA sensitivity of 0.57), QDA predicts many more incorrect top 10 placements than LDA does (1907 LDA false positives < 4853 QDA false positives). Because LDA has a better overall accuracy (0.9206 for LDA > 0.8779 for QDA), we think the LDA is a stronger model for prediction than QDA.



### Random Forest
We then tried our first non-parametrical model with Random Forest, which averages out the results of many Decision Trees to provide the lowest error rates across all of the permuations attempted.


#### Assumption Checking

Random Forest methods do not have the same level of assumption restrictions as many of the parametrical models we reviewed.  Fotunately, our data set had no null values to speak of which may have posed more of a problem.


```{r rf.model, results='hide', message=FALSE, include=FALSE, echo=FALSE, eval=TRUE}

# results='hide', message=FALSE, include=FALSE, echo=FALSE
rf_cols_to_remove = c("Id", "groupId", "matchId", "matchType", "DBNOs", "revives", "winPlacePerc")

train.rf <- train %>%
  dplyr::select(-rf_cols_to_remove) %>%
  mutate(kills  = kills * 100 / numGroups) %>% # Normalized Kills
  mutate(matchDuration = as.factor(ifelse(matchDuration < mean(matchDuration), "Low", "High"))) %>%
  mutate(top.10 = factor(top.10, labels = c("No", "Yes"))) 


test.rf <- test %>%
  dplyr::select(-rf_cols_to_remove) %>%
  mutate(kills  = kills * 100 / numGroups) %>% # Normalized Kills
  mutate(matchDuration = as.factor(ifelse(matchDuration < mean(matchDuration), "Low", "High"))) %>%
  mutate(top.10 = factor(top.10, labels = c("No", "Yes"))) 

set.seed(1234)
model.rf <- randomForest(as.factor(top.10) ~ ., 
                               data = train.rf, 
                               ntree = 500, 
                               mtry = 12, 
                               cutoff = c(0.4,1-0.40))


model.rf

```


```{r rf.plot, results='hide', message=FALSE, include=FALSE, echo=FALSE, eval=TRUE}


plot(model.rf)
varImp(model.rf)

```


```{r rf.predict, results='hide', message=FALSE, include=FALSE, echo=FALSE, eval=TRUE}


prd.rf.train <- predict(model.rf, train.rf)
prd.rf.test <- predict(model.rf, test.rf)

```


```{r rf.confusion.matrix, results='hide', message=FALSE, include=FALSE, echo=FALSE, eval=TRUE}
confusionMatrix(data=prd.rf.test,
                reference=test.rf$top.10, "Yes")
```




```{r rf.area.under.curve, results='hide', message=FALSE, include=FALSE, echo=FALSE, eval=TRUE}
plotRoc <- function(preds, truth) {
  pred <- prediction(preds, truth)
  roc.perf = performance(pred, measure = "tpr", x.measure = "fpr")
  auc.train <- performance(pred, measure = "auc")
  auc.train <- auc.train@y.values
  #Plot ROC
  par(mar=c(4,4,4,4))
  plot(roc.perf,main="Random Forest")
  abline(a=0, b= 1) #Ref line indicating poor performance
  text(x = .40, y = .6,paste("AUC = ", round(auc.train[[1]],3), sep = ""))
  table(test$top.10, useNA = "ifany")
}



plotRoc(as.integer(prd.rf.test),as.integer(test.rf$top.10))

# ?randomForest

```


#### Random Forest Results

The out-of-bag estimate of the error rate was 9.71% which seems to be consistent with our results against the test data not used to train the model with.

When running our model against the hold out test data set, we received the following performance metrics.

- Accuracy    : 0.8964  
- Sensitivity : 0.91811 
- Specificity : 0.89384  
- AUC         : 0.90598



#### Random Forest Conclusion

The Random Forest model required that the model be tuned with parameters better suited to the data set at hand.  

The following arguments were important to address bias/ variance issues that we observed when using the default settings.  Trial and error along with tuning libraries in `caret` and native to the `randomForest` library allowed us to better optimize the results.  We were able to get a roughly 90/90 split between our sensitivity and specificity.  Or rather, when it was in the top 10%, how often did we predict that it was and when it was in the bottom 90%, how often did we predict that it was respectively



* `ntree`      : Number of trees to grow
* `mtry`       : Number of variables randomly sampled as candidates at each split. 
* `cutoff`     : A vector of length equal to number of classes. 


## Comparing Competing Models


## Model Interpretation


## Conclusion


\newpage


# Appendix

## Exploratory Data Analysis

## Code

### LDA

#### LASSO

```{r code.lda.datapull, message=FALSE, include=TRUE, echo=TRUE, eval=TRUE}
# Reduce variables to relevant continuous variables
train1<-train[,c(5:6,8:10,12:15,19,21:27,29:30)]
test1<-test[,c(5:6,8:10,12:15,19,21:27,29:30)]
# train2 <- train1[,c(1,4:19)]
# test2 <- test1[,c(1,4:19)]
train2_alt <- train1[,c(1,4:17,19)]
test2_alt <- test1[,c(1,4:17,19)]
```

```{r code.lda.lasso,  message=FALSE, include=TRUE, echo=TRUE, eval=TRUE}

# Get data in format for LASSO
x=model.matrix(top.10~.,train2_alt)[,-1]
y=as.numeric(train2_alt$top.10)

xtest<-model.matrix(top.10~.,test2_alt)[,-1]
ytest<-as.numeric(test2_alt$top.10)

grid=10^seq(10,-2, length =100)
lasso.mod=glmnet(x,y,alpha=1, lambda =grid)

set.seed(23) #removes kills roadKills vehicleDestroys
cv.out=cv.glmnet(x,y,alpha=1,family="binomial") #alpha=1 performs LASSO
lda.lasso<-plot(cv.out)

# simplest model
lda.lasso.model.coef<-coef(cv.out, cv.out$lambda.1se)
lda.lasso.model.coef
```

```{r code.lda.train_final,  message=FALSE, include=TRUE, echo=TRUE, eval=TRUE}

# Removing the kills, roadKills, and vehicleDestroys as shown above
# Also removing rankPoints because 
# (a) the majority of the players in these data aren't ranked and 
# (b) the Kaggle descrption says "This ranking is inconsistent and is being deprecated in the API's next version"
train_final <- train2_alt[,c(-4,-8,-10,-13)]

```
#### Assumption Checking

```{r code.lda.assumptions.1,  message=FALSE, include=TRUE, echo=TRUE, eval=TRUE}

train_final_no <- train_final[which(train_final$top.10==0),]
train_final_yes <- train_final[which(train_final$top.10==1),]

# nrow(train_final)
# nrow(train_final_no)
# nrow(train_final_yes)

max_cols<-ncol(train_final)
no_matrix<-as.matrix(train_final_no[,-max_cols])
yes_matrix<-as.matrix(train_final_yes[,-max_cols])

no_hist<-multi.hist(no_matrix)
yes_hist<-multi.hist(yes_matrix)
# par(mfrow=c(1,1))

```

```{r code.lda.assumptions.2,  message=FALSE, include=TRUE, echo=TRUE, eval=TRUE}

#http://www.sthda.com/english/wiki/ggplot2-quick-correlation-matrix-heatmap-r-software-and-data-visualization
get_upper_tri <- function(cormat){
  cormat[lower.tri(cormat)]<- NA
  return(cormat)
}

custom_corr_plot <- function(cormat){
  
  upper_tri <- get_upper_tri(cormat)
  # Melt the correlation matrix
  melted_cormat <- melt(upper_tri, na.rm = TRUE)
  # Create a ggheatmap
  ggheatmap <- ggplot(melted_cormat, aes(Var2, Var1, fill = value))+
    geom_tile(color = "white")+
    scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                         midpoint = 0, limit = c(-1,1), space = "Lab", 
                         name="Pearson\nCorrelation") +
    theme_minimal()+ # minimal theme
    theme(axis.text.x = element_text(angle = 45, vjust = 1, 
                                     size = 12, hjust = 1))+
    coord_fixed()
  
  
  p<-ggheatmap + 
    geom_text(aes(Var2, Var1, label = value), color = "black", size = 4) +
    theme(
      axis.title.x = element_blank(),
      axis.title.y = element_blank(),
      panel.grid.major = element_blank(),
      panel.border = element_blank(),
      panel.background = element_blank(),
      axis.ticks = element_blank(),
      legend.justification = c(1, 0),
      legend.position = c(0.6, 0.7),
      legend.direction = "horizontal")+
    guides(fill = guide_colorbar(barwidth = 7, barheight = 1,
                                 title.position = "top", title.hjust = 0.5))
  
  return(p)
}

cormat <- round(cor(no_matrix),2)
no <- custom_corr_plot(cormat)

cormat <- round(cor(yes_matrix),2)
yes <- custom_corr_plot(cormat)

```

#### LDA Results

```{r code.lda.model,  message=FALSE, include=TRUE, echo=TRUE, eval=TRUE}

# Run LDA
lda <- lda(top.10 ~ . , data=train_final, prior=c(.9,.1))

lda

predict <- predict(lda,newdata=test)

test_final <- test
test_final$predcited_place <- as.vector(predict$class)
test_final$top.10<-as.factor(test_final$top.10)
test_final$predcited_place<-as.factor(test_final$predcited_place)

xtab<-table(test_final$predcited_place,test_final$top.10)
confusionMatrix(xtab, positive="1")

predict.posteriors <- as.data.frame(predict$posterior)

# Evaluate the model
pred <- prediction(predict.posteriors[,2], test$top.10)
roc.perf = performance(pred, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred, measure = "auc")
auc.train <- auc.train@y.values


# Plot
plot(roc.perf, main="ROC Curve - LDA")
abline(a=0, b= 1)
text(x = .25, y = .65 ,paste("AUC = ", round(auc.train[[1]],3), sep = ""))


```

#### QDA Results

```{r code.qda.model,  message=FALSE, include=TRUE, echo=TRUE, eval=TRUE}

qda <- qda(top.10 ~ . , data=train_final, prior=c(.9,.1))

qda

predict <- predict(qda,newdata=test)

test_final <- test
test_final$predcited_place <- as.vector(predict$class)
test_final$top.10<-as.factor(test_final$top.10)
test_final$predcited_place<-as.factor(test_final$predcited_place)
# test_final$predcited_place<-as.factor(test_final$predcited_place)
str(test_final)

xtab<-table(test_final$predcited_place,test_final$top.10)
confusionMatrix(xtab, positive="1")
# confusionMatrix(test_final$predcited_place, test_final$top.10)

predict.posteriors <- as.data.frame(predict$posterior)

# Evaluate the model
pred <- prediction(predict.posteriors[,2], test$top.10)
roc.perf = performance(pred, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred, measure = "auc")
auc.train <- auc.train@y.values


# Plot
plot(roc.perf, main="ROC Curve - QDA")
abline(a=0, b= 1)
text(x = .25, y = .65 ,paste("AUC = ", round(auc.train[[1]],3), sep = ""))


```

# References