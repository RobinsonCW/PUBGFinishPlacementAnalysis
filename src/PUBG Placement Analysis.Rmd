---
title: "PUBG Top 10% Placement Analysis"
author: "Chance Robinson, Allison Roderick and William Arnost"
date: |
  Master of Science in Data Science, Southern Methodist University, USA
lang: en-US
class: man
numbersections: true
encoding: UTF-8
bibliography: references.bib
biblio-style: apalike
output:
  bookdown::pdf_document2:
     citation_package: natbib
     keep_tex: true
     toc: false
header-includes:
   - \usepackage{amsmath}
   - \usepackage[utf8]{inputenc}
   - \usepackage[T1]{fontenc}
   - \usepackage{setspace}
   - \usepackage{hyperref}
   - \onehalfspacing
   - \setcitestyle{numbers,square,super}
   - \newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
editor_options:
  chunk_output_type: console
---
```{r, lib-read, results='hide', message=FALSE, include=FALSE, echo=FALSE, eval=TRUE}

# setwd("C:/Users/Chance/github/smu/PUBGFinishPlacementAnalysis/data")

# library imports
library(MASS)
library(tidyverse)
library(knitr)
library(kableExtra)

# Random Forest
library(randomForest)  
# downSample
library(caret)
# ROC Curves
library(ROCR)
library(pROC)
# outlier and leverage plots
library(olsrr)
# LDA
library(dplyr)
library(car)
library(RColorBrewer)
library(psych)
library(glmnet)
library(reshape2)

library(e1071)
library(corrplot)
library(GGally)
library(formattable)
#library(dlookr)
```


```{r, load-data, results='hide', message=FALSE, include=FALSE, echo=FALSE, eval=TRUE}

train <- read.csv("../data/pubg_solo_game_types_train_downsampled.csv", stringsAsFactors=FALSE)
test <- read.csv("../data/pubg_solo_game_types_test_full.csv", stringsAsFactors=FALSE)
solo <- read_csv("../data/pubg_solo_game_types.csv") %>% dplyr::select(-c("DBNOs","revives"))

# train <- read.csv("../data/pubg_solo_game_types_train_downsampled.csv", stringsAsFactors=FALSE)
# test <- read.csv("../data/pubg_solo_game_types_test_full.csv", stringsAsFactors=FALSE)
# solo <- read_csv("../data/pubg_solo_game_types.csv") %>% dplyr::select(-c("DBNOs","revives"))

# Chance
# train <- read.csv("C:/Users/Chance/github/smu/PUBGFinishPlacementAnalysis/data/datapubg_solo_game_types_train_downsampled.csv", stringsAsFactors=FALSE)
# test <- read.csv("C:/Users/Chance/github/smu/PUBGFinishPlacementAnalysis/data/pubg_solo_game_types_test_full.csv", stringsAsFactors=FALSE)

# Allison
# train <- read.csv("C:/Git/PUBGFinishPlacementAnalysis/data/pubg_solo_game_types_train_downsampled.csv", stringsAsFactors=FALSE)
# test <- read.csv("C:/Git/PUBGFinishPlacementAnalysis/data/pubg_solo_game_types_test_full.csv", stringsAsFactors=FALSE)

train <- train %>%
  mutate(top.10 = factor(top.10, labels = c("0", "1")))


test <- test %>%
  mutate(top.10 = factor(top.10, labels = c("0", "1")))

solo <- solo %>% mutate(
  top.10 = factor(top.10, labels = c("0", "1")),
DurationCut = as.factor(ifelse(matchDuration > 1600,1,0)),
kpCut = as.factor(ifelse(killPoints > 0,1,0)),
wpCut = as.factor(ifelse(winPoints > 0,1,0)),
rpCut = as.factor(ifelse(rankPoints > -1,1,0)),
boostCut = as.factor(ifelse(boosts >= 2,1,0)),
healCut = as.factor(ifelse(heals >= 2,1,0)),
walkCut = as.factor(ifelse(walkDistance >= 1250,1,0)),
weaponCut = as.factor(ifelse(weaponsAcquired >= 3,1,0)),
killCut = as.factor(ifelse(kills >= 2,1,0)),
assistCut = as.factor(ifelse(assists >= 2,1,0)),
damageCut = as.factor(ifelse(damageDealt >= 250,1,0)),
streakCut = as.factor(ifelse(killStreaks >= 1,1,0)),
killsPK = kills/(walkDistance/1000+1),
damageKill = kills/(damageDealt+1),
walkBin = cut(walkDistance, c(0,500,1000,1500,2000,2500,3000,3500,4000,4500,5000,20000))
)
```


# Introduction

PlayerUnknown's Battlegrounds, colloquially referred to as PUBG, is an online battle royale-style shooter that is one of the best-selling video games of all time. The gameâ€™s developer has created an API (application program interface) where others have collected data on over 65,000 matches and made that data available on the popular online competition site \href{https://www.kaggle.com/c/pubg-finish-placement-prediction/overview}{\textit{
Kaggle}} \cite{Kaggle2019}.  The project team will be utilizing logistic regression, linear and quadratic discriminant analyses, and random forest methods to attempt to create the best model for predicting whether or not an individual player in a solo match will place in the top 10 finishers or not.  


# Data Description

The source data is available on Kaggle.com under the competition PUBG Finish Placement Prediction. The files used in our analysis were transformed to fit the requirements of a binomial logistic regression classifier.  The data has also been pre-split into training and test files for consistency when comparing our different model types.  Additionally, as the percentage-based nature of the top 10% of players is inherently unbalanced, we've also downsampled the higher frequency data to match that over the lower frequency outcome of the top 10% of players.


`pubg_solo_game_types.csv`  

* Filtered for solo only game types

`pubg_solo_game_types_test_full.csv`  

* Pre-split for test data

`pubg_solo_game_types_train_full.csv`  

* Pre-split for train data without downsampling for the unbalanced response variable

`pubg_solo_game_types_train_downsampled.csv`  

* Pre-split for train data with downsampling for the unbalanced response variable




## Data Dictionary

Column Name      |Description
-----------------|----------------------------------------------------------------
DBNOs            |Number of enemy players knocked.
assists          |Number of enemy players this player damaged that were killed by teammates.
boosts           |Number of boost items used.
damageDealt      |Total damage dealt. Note: Self inflicted damage is subtracted.
headshotKills    |Number of enemy players killed with headshots.
heals            |Number of healing items used.
Id               |Players Id
killPlace        |Ranking in match of number of enemy players killed.
killPoints       |Kills-based external ranking of player. (Think of this as an Elo ranking where only kills matter.) If there is a value other than -1 in rankPoints, then any 0 in killPoints should be treated as a None.
killStreaks      |Max number of enemy players killed in a short amount of time.
kills            |Number of enemy players killed.
longestKill      |Longest distance between player and player killed at time of death. 
matchDuration    |Duration of match in seconds.
matchId          |ID to identify match. There are no matches that are in both the training and testing set.
matchType        |String identifying the game mode that the data comes from. 
rankPoints       |Elo-like ranking of player. 
revives          |Number of times this player revived teammates.
rideDistance     |Total distance traveled in vehicles measured in meters.
roadKills        |Number of kills while in a vehicle.
swimDistance     |Total distance traveled by swimming measured in meters.
teamKills        |Number of times this player killed a teammate.
vehicleDestroys  |Number of vehicles destroyed.
walkDistance     |Total distance traveled on foot measured in meters.
weaponsAcquired  |Number of weapons picked up.
winPoints        |Win-based external ranking of player. (Think of this as an Elo ranking where only winning matters.) If there is a value other than -1 in rankPoints, then any 0 in winPoints should be treated as a None.
groupId          |ID to identify a group within a match. If the same group of players plays in different matches, they will have a different groupId each time.
numGroups        |Number of groups we have data for in the match.
maxPlace         |Worst placement we have data for in the match. This may not match with numGroups, as sometimes the data skips over placements.
winPlacePerc     |This is a percentile winning placement, where 1 corresponds to 1st place, and 0 corresponds to last place in the match. (to be removed from our binomial classfier so as not to influence our predictive results)
top.10           |The target of prediction. This is a percentile winning placement, where 1 corresponds to a top 10% placement a 0 in the lower 90%.


\newpage


## Exploratory Data Analysis

The data full data set has 65,000 matches, each with data on the players in that match and statistics based on that individual match with only minor exceptions.  We have data on how much damage was dealt, how many kills a player got, and other information like distance travelled and weapons acquired. Below is a pairs plot for some of our core variables. 

```{r eda_pairs, message=FALSE, include=TRUE, echo=FALSE, eval=TRUE}
set.seed(1234)
sample <- sample_frac(solo, 0.01) 
pairsData <- sample %>% keep(is.numeric)
pairsData$top.10 <- sample$top.10

ggpairs(data = pairsData, mapping = aes(color = top.10),
        columns = c("assists" , "boosts" , "heals" , "weaponsAcquired" , "killPlace" , "walkDistance","top.10"))
```

A complete set of pair plots and box plots are available in the EDA appendix. There are a number of frequency tables in the DescriptiveTables script, but it was quite lengthy so it is not included here.

A number of variables are not useful for prediciton. Id, groupID, and matchId are identifiers. MatchType, DBNOs, and Revives are not relevant because we are focusing on the Solo match type. Finally, our response variable, top.10, is derived from winPlacePerc, so we cannot use it for prediction.  

### PCA

Principle Components Analysis can help us reduce numerical predictors to a few components. These groupings might help us find key variables to use for modeling

```{r PCA_1, message=FALSE, include=TRUE, echo=FALSE, eval=TRUE}
varsToDrop <- c("rankPoints","maxPlace","winPlacePerc")
moreDrop <- c("matchDuration","numGroups")
dataPCsolo <- solo %>% keep(is.numeric) %>% select(-varsToDrop,-moreDrop)

pc.result.solo <-prcomp(dataPCsolo,scale.=TRUE)
pc.scores.solo<-pc.result.solo$x
pc.scores.solo<-data.frame(pc.scores.solo)
pc.scores.solo$top.10 <- solo$top.10
pc.scores.solo$DurationCut <- solo$DurationCut
pc.scores.solo$kpCut <- solo$kpCut
pc.scores.solo$wpCut <- solo$wpCut
pc.scores.solo$rpCut <- solo$rpCut

pc.result.solo$rotation[,c(1:3)]
#Scree
#eigenvals<-(pc.result.solo$sdev)^2
#plot(1:length(eigenvals),eigenvals/sum(eigenvals),type="l",main="Scree Plot",ylab="Prop. Var. Explained",ylim=c(0,1))
#cumulative.prop<-cumsum(eigenvals/sum(eigenvals))
#lines(1:length(eigenvals),cumulative.prop,lty=2)


plotPCkp <- function(mydata) {
  sampleData <- sample_frac(mydata, 0.01)
  ggplot(data = sampleData, aes(x = PC1, y = PC2)) +
    geom_point(aes(col=kpCut), size=1, alpha = 0.5)+
    ggtitle("PCA of PUBG")
}
plotPC10 <- function(mydata) {
  sampleData <- sample_frac(mydata, 0.01)
  ggplot(data = sampleData, aes(x = PC1, y = PC2)) +
    geom_point(aes(col=top.10), size=1, alpha = 0.5)+
    ggtitle("PCA of PUBG")
}

set.seed(314159)
plotPCkp(pc.scores.solo)
plotPC10(pc.scores.solo)

```

We see kills based measures are pretty strong in PC1, and killPoints/Winpoints dominate PC2. Looking at the graph, we get a strange separation. The killPoints variable can explain this, with one group having killPoints = 0 and the other being killPoints > 0. Lets try it one more time without killpoints.
```{r PCA_2, message=FALSE, include=TRUE, echo=FALSE, eval=TRUE}

## Repeat the analysis without killPoints

dataPCsolo <- solo %>% keep(is.numeric) %>% select(-varsToDrop,-moreDrop,-c("killPoints","winPoints"))

pc.result.solo <-prcomp(dataPCsolo,scale.=TRUE)
pc.scores.solo<-pc.result.solo$x
pc.scores.solo<-data.frame(pc.scores.solo)
pc.scores.solo$top.10 <- solo$top.10
pc.scores.solo$DurationCut <- solo$DurationCut
pc.scores.solo$kpCut <- solo$kpCut
pc.scores.solo$wpCut <- solo$wpCut
pc.scores.solo$rpCut <- solo$rpCut

pc.result.solo$rotation[,c(1:3)]
#Scree
#eigenvals<-(pc.result.solo$sdev)^2
#plot(1:length(eigenvals),eigenvals/sum(eigenvals),type="l",main="Scree Plot",ylab="Prop. Var. Explained",ylim=c(0,1))
#cumulative.prop<-cumsum(eigenvals/sum(eigenvals))
#lines(1:length(eigenvals),cumulative.prop,lty=2)


plotPCkp <- function(mydata) {
  sampleData <- sample_frac(mydata, 0.01)
  ggplot(data = sampleData, aes(x = PC1, y = PC2)) +
    geom_point(aes(col=kpCut), size=1, alpha = 0.5)+
    ggtitle("PCA of PUBG")
}
plotPC10 <- function(mydata) {
  sampleData <- sample_frac(mydata, 0.01)
  ggplot(data = sampleData, aes(x = PC1, y = PC2)) +
    geom_point(aes(col=top.10), size=1, alpha = 0.5)+
    ggtitle("PCA of PUBG")
}

set.seed(314159)
plotPC10(pc.scores.solo)
```
We are still getting some separation, but it has much improved. We are not getting 100$ separation of the top.10 variable, so PCA alone is not giving us a clear answer as to what makes a top 10 player. 

# Objective I Analysis  

```{r Obj1_Data_Prep, results='hide', message=FALSE, include=FALSE, echo=FALSE, eval=TRUE}
library(tidyverse)
solo <- solo %>% mutate(
DurationCut = as.factor(ifelse(matchDuration > 1600,1,0)),
kpCut = as.factor(ifelse(killPoints > 0,1,0)),
wpCut = as.factor(ifelse(winPoints > 0,1,0)),
rpCut = as.factor(ifelse(rankPoints > -1,1,0)),
boostCut = as.factor(ifelse(boosts >= 2,1,0)),
healCut = as.factor(ifelse(heals >= 2,1,0)),
walkCut = as.factor(ifelse(walkDistance >= 1250,1,0)),
weaponCut = as.factor(ifelse(weaponsAcquired >= 3,1,0)),
killCut = as.factor(ifelse(kills >= 2,1,0)),
assistCut = as.factor(ifelse(assists >= 2,1,0)),
damageCut = as.factor(ifelse(damageDealt >= 250,1,0)),
streakCut = as.factor(ifelse(killStreaks >= 1,1,0)),
killsPK = kills/(walkDistance/1000+1),
damageKill = kills/(damageDealt+1),
walkBin = cut(walkDistance, c(0,500,1000,1500,2000,2500,3000,3500,4000,4500,5000,20000))
)

test_obj1 <- test %>% mutate(
  DurationCut = as.factor(ifelse(matchDuration > 1600,1,0)),
  kpCut = as.factor(ifelse(killPoints > 0,1,0)),
  wpCut = as.factor(ifelse(winPoints > 0,1,0)),
  rpCut = as.factor(ifelse(rankPoints > -1,1,0)),
  boostCut = as.factor(ifelse(boosts >= 2,1,0)),
  healCut = as.factor(ifelse(heals >= 2,1,0)),
  walkCut = as.factor(ifelse(walkDistance >= 1250,1,0)),
  weaponCut = as.factor(ifelse(weaponsAcquired >= 3,1,0)),
  killCut = as.factor(ifelse(kills >= 2,1,0)),
  assistCut = as.factor(ifelse(assists >= 2,1,0)),
  damageCut = as.factor(ifelse(damageDealt >= 250,1,0)),
  streakCut = as.factor(ifelse(killStreaks >= 1,1,0)),
  killsPK = kills/(walkDistance/1000+1),
  damageKill = kills/(damageDealt+1),
  walkBin = cut(walkDistance, c(0,500,1000,1500,2000,2500,3000,3500,4000,4500,5000,20000))
)

train_obj1 <- train %>% mutate(
  DurationCut = as.factor(ifelse(matchDuration > 1600,1,0)),
  kpCut = as.factor(ifelse(killPoints > 0,1,0)),
  wpCut = as.factor(ifelse(winPoints > 0,1,0)),
  rpCut = as.factor(ifelse(rankPoints > -1,1,0)),
  boostCut = as.factor(ifelse(boosts >= 2,1,0)),
  healCut = as.factor(ifelse(heals >= 2,1,0)),
  walkCut = as.factor(ifelse(walkDistance >= 1250,1,0)),
  weaponCut = as.factor(ifelse(weaponsAcquired >= 3,1,0)),
  killCut = as.factor(ifelse(kills >= 2,1,0)),
  assistCut = as.factor(ifelse(assists >= 2,1,0)),
  damageCut = as.factor(ifelse(damageDealt >= 250,1,0)),
  streakCut = as.factor(ifelse(killStreaks >= 1,1,0)),
  killsPK = kills/(walkDistance/1000+1),
  damageKill = kills/(damageDealt+1),
  walkBin = cut(walkDistance, c(0,500,1000,1500,2000,2500,3000,3500,4000,4500,5000,20000))
)

## References to groups of columns
varGroupKill <- c("damageDealt","headshotKills","kills","killStreaks","roadKills","vehicleDestroys","weaponsAcquired")
varGroupTeam <- c("assists","boosts","heals","teamKills")
varGroupDistance <- c("longestKill","rideDistance","swimDistance","walkDistance")
varGroupPoints <- c("killPoints","winPoints","rankPoints","winPlacePerc","matchDuration")

## Helper Functions
plotRoc <- function(preds, truth) {
  pred <- prediction(preds, truth)
  roc.perf = performance(pred, measure = "tpr", x.measure = "fpr")
  auc.train <- performance(pred, measure = "auc")
  auc.train <- auc.train@y.values
  
  #Plot ROC
  par(mar=c(4,4,4,4))
  plot(roc.perf,main="Ordinary Logistic")
  abline(a=0, b= 1) #Ref line indicating poor performance
  text(x = .40, y = .6,paste("AUC = ", round(auc.train[[1]],3), sep = ""))
  #table(test_obj1$top.10, useNA = "ifany")
}
scoreMod <- function(model.out) {
  model.pred <- predict(model.out, newdata = test_obj1, type = "response")
  plotRoc(model.pred,test_obj1$top.10)
  confusionMatrix(as.factor(as.numeric(model.pred > 0.5)), test_obj1$top.10, positive = "1")
}
```


## Problem Statement  

We would like to predict if a player finished in the top 10 of a PUBG match based on their stats during the match. Our response variable is top.10, which is 1 if the player finished in the top 10 and 0 if they did not. We will start with a logistic regression model before moving into more complex methods.  

## Establishing the Initial Model  

During EDA, we discovered a number of variables that would not be useful for prediction, either because they were ID columns or match characteristics that were not relevant to individual player performance. Removing those, our first model contains all the remaining variables.  
```{r obj1_model1, message=FALSE, include=TRUE, echo=FALSE, eval=TRUE}
## Logistic Model 1 - "The Kitchen Sink"
model.main <- glm(top.10~assists + boosts + heals + teamKills + weaponsAcquired +
                    damageDealt + headshotKills + kills + killStreaks + roadKills + vehicleDestroys + 
                    killPlace + killPoints  + rankPoints + winPoints +
                    longestKill + swimDistance + rideDistance + walkDistance,
                  data= train_obj1, family = binomial(link="logit"))
summary(model.main)
#vif(model.main) ## Kill points and win points super correlated
#scoreMod(model.main)
```

We can see some insignificant variables, and the vif (Appendix: Logistic Model) will show some correlation. This model performance of this model is as follows: 

Model   |AUC     |Sensitivity |Specificity | Error Rate (1 - Accuracy)
--------|--------|------------|------------|--------------------------
Initial | 0.941  | 0.891      | 0.843      | 15.2%


## Variable Selection 

Next, we use lasso to narrow these down to the most important variables. First, we used cv.glmnet to search for the ideal value of lambda for the lasso regression (Appendix: Logistic Regression). We selected the lambda.1se value, which will hopefully eliminate the most variables. Using that value for lambda, lasso produced this output:

```{r obj1_lasso, message=FALSE, include=TRUE, echo=FALSE, eval=TRUE}
set.seed(1234)
x <- model.matrix(top.10~assists + boosts + heals + teamKills + weaponsAcquired +
                    damageDealt + headshotKills + kills + killStreaks + roadKills + vehicleDestroys + 
                    killPlace + killPoints  + rankPoints + winPoints +
                    longestKill + swimDistance + rideDistance + walkDistance, data=train_obj1)
cv.lasso <- cv.glmnet(x, train_obj1$top.10, alpha = 1, family = "binomial")
plot(cv.lasso)

#cv.lasso$lambda.min
#coef(cv.lasso, cv.lasso$lambda.min)
#cv.lasso$lambda.1se
#coef(cv.lasso, cv.lasso$lambda.1se)


model.lasso <- glmnet(x, train_obj1$top.10, alpha = 1, family = "binomial",
                      lambda = cv.lasso$lambda.1se)

coef(model.lasso)

#x.test <- model.matrix(top.10~assists + boosts + heals + teamKills + weaponsAcquired +
#                    damageDealt + headshotKills + kills + killStreaks + roadKills + vehicleDestroys + 
#                    killPlace + killPoints  + rankPoints + winPoints +
#                    longestKill + swimDistance + rideDistance + walkDistance, data=test_obj1)

#lasso.pred <- predict(model.lasso, newx = x.test)
#plotRoc(lasso.pred,test_obj1$top.10)
#confusionMatrix(as.factor(as.numeric(lasso.pred > 0.5)), test_obj1$top.10, positive = "1")
```

## Final Iterations

The selection eliminated damageDealt, headshotKills, and killPoints. We created a model using just the variables from lasso and found two issues. First, winPoints and rankPoints have high VIFs. Removing them individually found that when one is missing, the other is no longer significant. We decided to remove them both. Second, the cooksD plot shows 3 potentially influential points. All three points have high values ( > p99 threshold) for roadKills and killStreaks, and I also found through adding and removing variables that swimDistance contributes influential points. We chose to remove these variables, as a very small part of the population has high values in these metrics, and it negatively effects the model.  

```{r obj1_final, message=FALSE, include=TRUE, echo=FALSE, eval=TRUE}
model.final.lasso <- glm(top.10~assists + boosts + heals + teamKills + weaponsAcquired +
                    kills + killStreaks + roadKills + vehicleDestroys + 
                    killPlace + rankPoints + winPoints +
                    longestKill + swimDistance + rideDistance + walkDistance,
                  data= train_obj1, family = binomial(link="logit"))
#summary(model.final.lasso)
#vif(model.final.lasso) ## Kill points and win points super correlated
```

Here we can see influential points from the Cook's D chart.  

```{r obj1_finalb, message=FALSE, include=TRUE, echo=FALSE, eval=TRUE}
plot(model.final.lasso, which = 4, id.n = 3)
#scoreMod(model.final.lasso)


#model.data %>% top_n(3, .cooksd)

## trying to remove some of the influential points and correlated vars
  #No rankpoints

model.inf1 <- glm(top.10~assists + boosts + heals + teamKills + weaponsAcquired +
                           kills + killStreaks + roadKills + vehicleDestroys + 
                           killPlace + winPoints +
                           longestKill + swimDistance + rideDistance + walkDistance,
                         data= train_obj1, family = binomial(link="logit"))
#summary(model.inf1)
#vif(model.inf1)
##winpoints no longer significant  
#No winpoints
model.inf2 <- glm(top.10~assists + boosts + heals + teamKills + weaponsAcquired +
                    kills + killStreaks + roadKills + vehicleDestroys + 
                    killPlace + rankPoints +
                    longestKill + swimDistance + rideDistance + walkDistance,
                  data= train_obj1, family = binomial(link="logit"))
#summary(model.inf2)
#  vif(model.inf2)
##rankpoints not significant

##neither

model.inf2a <- glm(top.10~assists + boosts + heals + teamKills + weaponsAcquired +
                    kills + vehicleDestroys + killPlace + killStreaks + roadKills + swimDistance +
                    longestKill + rideDistance + walkDistance,
                  data= train_obj1, family = binomial(link="logit"))
#summary(model.inf2a)
#vif(model.inf2a)

#plot(model.inf2a, which = 4, id.n = 3)

## removing killStreaks + roadKills + swimDistance
model.inf3 <- glm(top.10~assists + boosts + heals + teamKills + weaponsAcquired +
                  kills + vehicleDestroys + killPlace + 
                  longestKill + rideDistance + walkDistance,
                data= train_obj1, family = binomial(link="logit"))
#summary(model.inf3)
#vif(model.inf3)
```

\newpage

After removing roadKills , killStreaks, and swimDistance  

```{r obj1_finalc, message=FALSE, include=TRUE, echo=FALSE, eval=TRUE}
plot(model.inf3, which = 4, id.n = 3)

#scoreMod(model.inf3)
```

Final Model Coefficients  

```{r obj1_finald, message=FALSE, include=TRUE, echo=FALSE, eval=TRUE}
#assists + boosts + heals + weaponsAcquired + kills + walkDistance
model.inf4 <- glm(top.10~assists + boosts + heals + weaponsAcquired + killPlace + walkDistance ,
                  data= train_obj1, family = binomial(link="logit"))
summary(model.inf4)
#vif(model.inf4)
#plot(model.inf4, which = 4, id.n = 3)

#scoreMod(model.inf4)
```

## Comparing Competing Models

Model   |AUC     |Sensitivity |Specificity | Error Rate (1 - Accuracy)
--------|--------|------------|------------|--------------------------
Initial | 0.941  | 0.891      | 0.843      | 15.2%
Lasso   | 0.941  | 0.892      | 0.843      | 15.2%
Inf. Points   | 0.936  | 0.892      | 0.842      | 15.3%
Final   | 0.932  | 0.863      | 0.841      | 15.6%


## Model Interpretation

We prefer the final model because it simplifies the previous versions without significant loss in accuracy.  The variable interpretations are found in the table below.

Variable        | Interpretation
----------------|--------------------------
assists         | The log-odds of placing in the top 10 increase by 0.6225 for each additional assist
boosts          | The log-odds of placing in the top 10 increase by 0.3067 for each additional boost
heals           | The log-odds of placing in the top 10 decrease by -0.0228 for each additional heal
weaponsAcquired | The log-odds of placing in the top 10 increase by 0.0790 for each additional weapon acquired
killPlace       | The log-odds of placing in the top 10 increase by 0.0622 for each additional rank in kills (lower is better for killPlace)
walkDistance    | The log-odds of placing in the top 10 increase by 0.0007 for each additional meter walked


## Conclusion

Players who do well in kill placement and who travel far tend to win games. Travel distance probably is a proxy for time alive, as players eliminated early would probably not travel far. Acquiring in game items like boosts or weapons also help. The negative coeffient on heals is odd, perhaps indicating that taking more damage is detrimental to placement.  We will try to improve on the final model using more complex methods.

\newpage


# Objective II Analysis


## Question of Interest

As we stated in the Problem Statement, we would like to predict if a player finished in the top 10 of a PUBG match based on their stats during the match. The next three methods of prediction we will present areLogistic Regression using more complicated terms than what were presented in Objective 1, Linear Discriminant Analysis (LDA), and Random Forest. These different techniques are all going to help us answer the question: What model is has the highest predicitve accuracy for classifying players as top 10 finishers?


## Model Selection

### Logistic Regression with complex terms

#### Objective 

In the first logistic regression we used terms that were easier to interpret, now we will add more complex terms. The final model from the previous section was:

top.10 ~ assists + boosts + heals + weaponsAcquired + killPlace + walkDistance

Additional terms considered in the more complex model are as follows:
* Dichotomized variables: Two level versions of the original variables, with cutoffs determined by EDA to try to separate top.10 from the rest of the group
\begin{itemize}
  \item Binned Variables: These have more levels than the dichotomized versions, to try to capture more complexity
  \item Ratios: Key variables are compared against each other in ratio format. For instance, does damage dealt per kill make a difference?
  \item	Interactions: multiplicative relationships between key variables. For instance, does increased damage matter if the player is above the boosts cut point?
  \item	Full definitions of new variables can be found in Appendix: Complex Logistic Regression
\end{itemize}

#### Model Selection

We ran a total of 8 additional logistic models, with the following results:

Model   |AUC     |Sensitivity |Specificity | Error Rate (1 - Accuracy) | Warning
--------|--------|------------|------------|-------------------------------------
Model 01 | 0.932 | 0.873 |  0.836 | 16.0% | No
Model 02 | 0.932 | 0.862 |  0.841 | 15.7% | No
Model 03 | 0.932 | 0.885 |  0.832 | 16.2% | No
Model 04 | 0.936 | 0.889 |  0.827 | 16.6% | No
Model 05 | 0.932 | 0.864 |  0.841 | 15.6% | No
Model 06 | 0.939 | 0.888 |  0.836 | 15.8% | prediction from a rank-deficient fit may be misleading
Model 07 | 0.936 | 0.895 |  0.840 | 15.4% | No
Model 08 | 0.936 | 0.894 |  0.838 | 15.6% | fitted probabilities numerically 0 or 1 occurred 
Obj1 Final   | 0.932  | 0.863      | 0.841      | 15.6% | No

The most successful model was number 7, which had the best accuracy with no warnings. The model coefficients and performance are in the figures below:  

```{r obj2_logistic1, message=FALSE, include=TRUE, echo=FALSE, eval=TRUE}
model.07 <- glm(top.10~boosts + assists + weaponsAcquired + killPlace + walkDistance + killsPK,
                data= train_obj1, family = binomial(link="logit"))
summary(model.07)
scoreMod(model.07)
```

The additional variable for this model is killsPK, or kills per kilometer. It is a ratio designed to separate players who are aggressive and engaging other players during the match vs. those who travel far but play a more stealthy strategy. The model suggests each additional unit of killsPK would decrease the log odds of entering the top 10 by 0.794

#### Conclusions for Logistic Regression

Of the models we tried, none achieved an increase in accuracy that would justify the additional complexity. We also ran into issues with complex variables. The rank deficient error suggest some values of the combination of two categorical variables did not have any observations assigned. In this case it would make sense to code a special variable to account for this. The other warning suggests some combination of variables allowed for perfrect seperation of top.10, but which variables was not clear from the EDA. For logistic regression, the best model we found was the one at the end of objective one.

### Linear Discriminant Analysis
Our next prediction tool is Linear Discriminant Analysis (LDA) for classifying the player as Top 10 or not. We have taken a subset of the continuous variables from our EDA to build the LDA off of. Before running the LDA, we will cover two things: a LASSO call to eliminate less important variables and assumption checking.

#### LASSO

The LASSO call plus manual variable selection reduced the predictors considered for the LDA model to: boosts, heals, killPlace, killStreaks, longestKill, matchDuration, rideDistance, swimDistance, teamKills, walkDistance, weaponsAcquired. See see the Appendix - LDA for the output of the LASSO call.

```{r lda.datapull, results='hide', message=FALSE, include=FALSE, echo=FALSE, eval=TRUE}
# Reduce variables to relevant continuous variables
train1<-train[,c(5:6,8:10,12:15,19,21:27,29:30)]
test1<-test[,c(5:6,8:10,12:15,19,21:27,29:30)]
# train2 <- train1[,c(1,4:19)]
# test2 <- test1[,c(1,4:19)]
train2_alt <- train1[,c(1,4:17,19)]
test2_alt <- test1[,c(1,4:17,19)]
```

```{r lda.lasso, results='hide', message=FALSE, include=FALSE, echo=FALSE, eval=TRUE}

# Get data in format for LASSO
x=model.matrix(top.10~.,train2_alt)[,-1]
y=as.numeric(train2_alt$top.10)

xtest<-model.matrix(top.10~.,test2_alt)[,-1]
ytest<-as.numeric(test2_alt$top.10)

grid=10^seq(10,-2, length =100)
lasso.mod=glmnet(x,y,alpha=1, lambda =grid)

set.seed(23) #removes kills roadKills vehicleDestroys
cv.out=cv.glmnet(x,y,alpha=1,family="binomial") #alpha=1 performs LASSO
lda.lasso<-plot(cv.out)

# simplest model
lda.lasso.model.coef<-coef(cv.out, cv.out$lambda.1se)
# lda.lasso.model.coef
```

```{r lda.train_final, results='hide', message=FALSE, include=FALSE, echo=FALSE, eval=TRUE}

# Removing the kills, roadKills, and vehicleDestroys as shown above
# Also removing rankPoints because 
# (a) the majority of the players in these data aren't ranked and 
# (b) the Kaggle descrption says "This ranking is inconsistent and is being deprecated in the API's next version"
train_final <- train2_alt[,c(-4,-8,-10,-13)]

```
#### Assumption Checking

LDA performs optimally when the assumptions of MANOVA are met. That is, 

1. The predictors are normally distributed for each class of the response.

2. The covariance matrices for each class of the response are homogeneous.

When we check the first assumption for the predictors that are to be included in the LDA model, we see that the assumption is not met, as shown in the histograms of the variables as shown in the Appendix - LDA. Most of the predictors are right skewed. The variable matchDuration is bimodal. To remedy this, we tried transforming the variables, but it did not help our overall prediction accuracy. However, because issues of normality exist, we will explore QDA as well as LDA.

```{r lda.assumptions.1, results='hide', message=FALSE, include=FALSE, echo=FALSE, eval=TRUE}

train_final_no <- train_final[which(train_final$top.10==0),]
train_final_yes <- train_final[which(train_final$top.10==1),]

# nrow(train_final)
# nrow(train_final_no)
# nrow(train_final_yes)

max_cols<-ncol(train_final)
no_matrix<-as.matrix(train_final_no[,-max_cols])
yes_matrix<-as.matrix(train_final_yes[,-max_cols])

no_hist<-multi.hist(no_matrix)
yes_hist<-multi.hist(yes_matrix)
# par(mfrow=c(1,1))

```

We also checked the homogeneity of correlation matrices, shown in the Appendix - LDA. We find that, overall, there are no major departures from homogeneity. The variables walkDistance and killPlace show the greatest deviances from homogenous correlations between bottom 90 and top 10 placements. Consequently, we tried removing those variables from the model. However, removing those variables reduced our prediction accuracy.

Thus, we will proceed with the variables selected and see if LDA or QDA performs better.

```{r lda.assumptions.2, results='hide', message=FALSE, include=FALSE, echo=FALSE, eval=TRUE}

#http://www.sthda.com/english/wiki/ggplot2-quick-correlation-matrix-heatmap-r-software-and-data-visualization
get_upper_tri <- function(cormat){
  cormat[lower.tri(cormat)]<- NA
  return(cormat)
}

custom_corr_plot <- function(cormat){
  
  upper_tri <- get_upper_tri(cormat)
  # Melt the correlation matrix
  melted_cormat <- melt(upper_tri, na.rm = TRUE)
  # Create a ggheatmap
  ggheatmap <- ggplot(melted_cormat, aes(Var2, Var1, fill = value))+
    geom_tile(color = "white")+
    scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                         midpoint = 0, limit = c(-1,1), space = "Lab", 
                         name="Pearson\nCorrelation") +
    theme_minimal()+ # minimal theme
    theme(axis.text.x = element_text(angle = 45, vjust = 1, 
                                     size = 12, hjust = 1))+
    coord_fixed()
  
  
  p<-ggheatmap + 
    geom_text(aes(Var2, Var1, label = value), color = "black", size = 4) +
    theme(
      axis.title.x = element_blank(),
      axis.title.y = element_blank(),
      panel.grid.major = element_blank(),
      panel.border = element_blank(),
      panel.background = element_blank(),
      axis.ticks = element_blank(),
      legend.justification = c(1, 0),
      legend.position = c(0.6, 0.7),
      legend.direction = "horizontal")+
    guides(fill = guide_colorbar(barwidth = 7, barheight = 1,
                                 title.position = "top", title.hjust = 0.5))
  
  return(p)
}

cormat <- round(cor(no_matrix),2)
lda.no <- custom_corr_plot(cormat)

cormat <- round(cor(yes_matrix),2)
lda.yes <- custom_corr_plot(cormat)

```

#### LDA Results

LDA has a prediction accuracy of 0.9206, with a sensitivity of 0.57479 and a specificity of 0.96099. The area under the ROC curve is 0.941. Output of the LDA call and the ROC curve are shown in Appendix - LDA. The confusion matrix is shown below for comparison with QDA.

```{r lda.model,  message=FALSE, include=TRUE, echo=FALSE, eval=TRUE}

# Run LDA
lda <- lda(top.10 ~ . , data=train_final, prior=c(.9,.1))

# lda

predict <- predict(lda,newdata=test)

test_final <- test
test_final$predcited_place <- as.vector(predict$class)
test_final$top.10<-as.factor(test_final$top.10)
test_final$predcited_place<-as.factor(test_final$predcited_place)

xtab<-table(test_final$predcited_place,test_final$top.10)
confusionMatrix(xtab, positive="1")

predict.posteriors <- as.data.frame(predict$posterior)

# Evaluate the model
pred <- prediction(predict.posteriors[,2], test$top.10)
roc.perf = performance(pred, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred, measure = "auc")
auc.train <- auc.train@y.values


# Plot
# plot(roc.perf, main="ROC Curve - LDA")
# abline(a=0, b= 1)
# text(x = .25, y = .65 ,paste("AUC = ", round(auc.train[[1]],3), sep = ""))


```

#### QDA Results

QDA has a prediction accuracy of 0.8779, with a sensitivity of 0.68262 and a specificity of 0.90071. The area under the ROC curve is 0.912 .Output of the QDA call and the ROC curve are shown in Appendix - LDA. The confusion matrix is shown below for comparison with LDA.

```{r qda.model, message=FALSE, include=TRUE, echo=FALSE, eval=TRUE}


qda <- qda(top.10 ~ . , data=train_final, prior=c(.9,.1))

# qda

predict.qda <- predict(qda,newdata=test)

# test_final <- test
test_final$predcited_place.qda <- as.vector(predict$class)
# test_final$top.10<-as.factor(test_final$top.10)
test_final$predcited_place.qda<-as.factor(test_final$predcited_place.qda)
# test_final$predcited_place<-as.factor(test_final$predcited_place)
# str(test_final)

xtab.qda<-table(test_final$predcited_place.qda,test_final$top.10)
confusionMatrix(xtab.qda, positive="1")

predict.posteriors.qda <- as.data.frame(predict.qda$posterior)

# Evaluate the model
pred.qda <- prediction(predict.posteriors.qda[,2], test$top.10)
roc.perf.qda = performance(pred.qda, measure = "tpr", x.measure = "fpr")
auc.train.qda <- performance(pred.qda, measure = "auc")
auc.train.qda <- auc.train.qda@y.values


# Plot
# plot(roc.perf.qda, main="ROC Curve - QDA")
# abline(a=0, b= 1)
# text(x = .25, y = .65 ,paste("AUC = ", round(auc.train[[1]],3), sep = ""))


```

#### LDA Conclusion
Although the QDA is better at predicting the top 10 placements that were true top 10 than LDA (QDA sensitivity of 0.68 > LDA sensitivity of 0.57), QDA predicts many more incorrect top 10 placements than LDA does (1907 LDA false positives < 4853 QDA false positives). Because LDA has a better overall accuracy (0.9206 for LDA > 0.8779 for QDA), we think the LDA is a stronger model for prediction than QDA.


\newpage

### Random Forest
We then tried our first non-parametrical model with Random Forest, which averages out the results of many Decision Trees to provide the lowest error rates across all of the permuations attempted.


#### Assumption Checking

Random Forest methods do not have the same level of assumption restrictions as many of the parametrical models we reviewed.  Fotunately, our data set had no null values to speak of which may have posed more of a problem.


```{r rf.model, results='hide', message=FALSE, include=FALSE, echo=FALSE, eval=TRUE}

# results='hide', message=FALSE, include=FALSE, echo=FALSE
rf_cols_to_remove = c("Id", "groupId", "matchId", "matchType", "DBNOs", "revives", "winPlacePerc")

train.rf <- train %>%
  dplyr::select(-rf_cols_to_remove) %>%
  mutate(kills  = kills * 100 / numGroups) %>% # Normalized Kills
  mutate(matchDuration = as.factor(ifelse(matchDuration < mean(matchDuration), "Low", "High"))) %>%
  mutate(top.10 = factor(top.10, labels = c("No", "Yes"))) 


test.rf <- test %>%
  dplyr::select(-rf_cols_to_remove) %>%
  mutate(kills  = kills * 100 / numGroups) %>% # Normalized Kills
  mutate(matchDuration = as.factor(ifelse(matchDuration < mean(matchDuration), "Low", "High"))) %>%
  mutate(top.10 = factor(top.10, labels = c("No", "Yes"))) 

set.seed(1234)
model.rf <- randomForest(as.factor(top.10) ~ ., 
                               data = train.rf, 
                               ntree = 500, 
                               mtry = 12, 
                               cutoff = c(0.4,1-0.40))


model.rf

```

#### Error Rates by No. of Trees

The model could likely be tuned further as you can see there's no additional payoff from having the `ntree` value set so high.  In fact, it could be harming the performance by setting it to such a high value as you can see the error rates starting to trend upwards at the far-right edge of the graph.

```{r rf.model.plot, fig.align = "center", message=FALSE, echo=FALSE, eval=TRUE}

plot(model.rf)

```

\newpage

#### Variable Importance

Similar features were confirmed as having the best predictive outcomes from the variable importance output.  (i.e...Kill Place, Walking Distance, etc..)

```{r rf.variable.importance, message=FALSE, echo=FALSE, eval=TRUE}

# varImp(model.rf)

rf.imp.variables <-data.frame(varImp(model.rf))

# rf.imp.variables[order(rf.imp.variables$Overall,decreasing = T),]

kable(data.frame('Column Name' = rownames(rf.imp.variables), 
                 'Overall Score'= rf.imp.variables$Overall),
      "latex", booktabs = T)  %>%
  kable_styling(position = "center")

```


```{r rf.predict, results='hide', message=FALSE, include=FALSE, echo=FALSE, eval=TRUE}


prd.rf.train <- predict(model.rf, train.rf)
prd.rf.test <- predict(model.rf, test.rf)

```


```{r rf.confusion.matrix, results='hide', message=FALSE, include=FALSE, echo=FALSE, eval=TRUE}
confusionMatrix(data=prd.rf.test,
                reference=test.rf$top.10, "Yes")
```




```{r rf.area.under.curve, results='hide', message=FALSE, include=FALSE, echo=FALSE, eval=TRUE}
plotRoc <- function(preds, truth) {
  pred <- prediction(preds, truth)
  roc.perf = performance(pred, measure = "tpr", x.measure = "fpr")
  auc.train <- performance(pred, measure = "auc")
  auc.train <- auc.train@y.values
  #Plot ROC
  par(mar=c(4,4,4,4))
  plot(roc.perf,main="Random Forest")
  abline(a=0, b= 1) #Ref line indicating poor performance
  text(x = .40, y = .6,paste("AUC = ", round(auc.train[[1]],3), sep = ""))
  table(test$top.10, useNA = "ifany")
}



plotRoc(as.integer(prd.rf.test),as.integer(test.rf$top.10))

# ?randomForest

```

\newpage

#### Random Forest Results

The out-of-bag estimate of the error rate was 9.71% which wasconsistent with our finding from classifying the test data not used to train the model with.  This should allow us to better guage what real-world performance would be like with new data to apply the model to.

When running our model against the hold out test data set, we received the following performance metrics.


Model   |AUC     |Sensitivity |Specificity | Error Rate (1 - Accuracy)
--------|--------|------------|------------|--------------------------
RF      | 0.963  | 0.918      | 0.894      | 10.4%



#### Random Forest Conclusion

The Random Forest approach required that the model be tuned with parameters better suited to the provided data set.  

The following arguments were important to address bias/ variance issues that were observed when using the default settings.  Trial and error techniques along with tuning libraries in `caret` and native to the `randomForest` library allowed us to better optimize the results.  We were able to get a roughly 90/90 split between our sensitivity and specificity performance measures.  Or rather, when it was in the top 10%, the model was able to accurately predict that it was roughly 90% of the time.  The same could be said for the specificity metric.

From the library documentation:

* `ntree`      : Number of trees to grow
* `mtry`       : Number of variables randomly sampled as candidates at each split. 
* `cutoff`     : A vector of length equal to number of classes. 

## Overall Comparison of Models and Conclusion
Model   |AUC     |Sensitivity |Specificity | Error Rate (1 - Accuracy)
--------|--------|------------|------------|--------------------------
Logistic regression (simple - final)   | 0.932  | 0.863      | 0.841      | 15.6%
Logistic regression (complex - model 7)   | 0.936  | 0.895      | 0.840      | 15.4%
LDA   | 0.941  | 0.575      | 0.961      | 7.94%
Random forest   | 0.963  | 0.918      | 0.894      | 10.4%

The logistic regression models both performed well on all four metrics of interest (AUC, sensitivity, specificity, and error rate). Due to the fact that the simple logistic regression model is easier to interpret than the complex version, we prefer this simple model over the complex one. LDA had the highest AUC and lowest error rate. However, due to its low sensitivity, it was the worst model in terms of answering our question of interest: how can we best predict top 10 finishers. Random forest outperformed the logistic models in most categories and perhaps further hyperparameter tuning could improve this model.  

However, due to its overall simplicity and easy interpretability, our preferred model is the simple logistic regression model. Logistic regression in this format allows us to specify the log-odds of placing in the top 10, as shown in the Model Interpretation section of Objective 1.
Based on the information we gathered from all four methods of modeling this data, we recommend to PUBG players to rank highly in the number of kills per match, walk a lot (or stay in the match long enough to walk a lot), and use boost items to your advantage.

\newpage


# Appendix

## Exploratory Data Analysis

### Pairs Plots

```{r appendix_eda_pairs, message=FALSE, include=TRUE, echo=FALSE, eval=TRUE}
ggpairs(data = pairsData, mapping = aes(color = top.10),
        columns = c(varGroupPoints,"top.10"))
ggpairs(data = pairsData, mapping = aes(color = top.10),
        columns = c(varGroupDistance,"top.10"))
ggpairs(data = pairsData, mapping = aes(color = top.10),
        columns = c(varGroupKill,"top.10"))
ggpairs(data = pairsData, mapping = aes(color = top.10),
        columns = c(varGroupTeam,"top.10"))## Model Apendicies
```

### Box Plots 

```{r appendix_eda_box, message=FALSE, include=TRUE, echo=FALSE, eval=TRUE}
ggplot(solo,aes(x=top.10, y=walkDistance/1000, fill=top.10)) + geom_boxplot()
ggplot(solo,aes(x=top.10, y=swimDistance, fill=top.10)) + geom_boxplot()
ggplot(solo,aes(x=top.10, y=rideDistance, fill=top.10)) + geom_boxplot()
ggplot(solo,aes(x=top.10, y=longestKill, fill=top.10)) + geom_boxplot()

ggplot(solo,aes(x=top.10, y=killsPK, fill=top.10)) + geom_boxplot()
ggplot(solo,aes(x=top.10, y=damageKill, fill=top.10)) + geom_boxplot()

ggplot(solo,aes(x=top.10, y=heals, fill=top.10)) + geom_boxplot()
ggplot(solo,aes(x=top.10, y=kills, fill=top.10)) + geom_boxplot()
ggplot(solo,aes(x=top.10, y=damageDealt, fill=top.10)) + geom_boxplot()
ggplot(solo,aes(x=top.10, y=headshotKills, fill=top.10)) + geom_boxplot()
ggplot(solo,aes(x=top.10, y=killPoints, fill=top.10)) + geom_boxplot()
ggplot(solo,aes(x=top.10, y=winPoints, fill=top.10)) + geom_boxplot()
ggplot(solo,aes(x=top.10, y=killStreaks, fill=top.10)) + geom_boxplot()
ggplot(solo,aes(x=top.10, y=weaponsAcquired, fill=top.10)) + geom_boxplot()
ggplot(solo,aes(x=top.10, y=vehicleDestroys, fill=top.10)) + geom_boxplot()
```
### Logistic Regression

#### Original logistic model with VIF  
```{r appendix_obj1_model1, message=FALSE, include=TRUE, echo=FALSE, eval=TRUE}
## Logistic Model 1 - "The Kitchen Sink"
model.main <- glm(top.10~assists + boosts + heals + teamKills + weaponsAcquired +
                    damageDealt + headshotKills + kills + killStreaks + roadKills + vehicleDestroys + 
                    killPlace + killPoints  + rankPoints + winPoints +
                    longestKill + swimDistance + rideDistance + walkDistance,
                  data= train_obj1, family = binomial(link="logit"))
summary(model.main)
vif(model.main) ## Kill points and win points super correlated
scoreMod(model.main)
```

#### Full Lasso Variable Reduction

```{r appendix_obj1_lasso, message=FALSE, include=TRUE, echo=FALSE, eval=TRUE}
set.seed(1234)
x <- model.matrix(top.10~assists + boosts + heals + teamKills + weaponsAcquired +
                    damageDealt + headshotKills + kills + killStreaks + roadKills + vehicleDestroys + 
                    killPlace + killPoints  + rankPoints + winPoints +
                    longestKill + swimDistance + rideDistance + walkDistance, data=train_obj1)
cv.lasso <- cv.glmnet(x, train_obj1$top.10, alpha = 1, family = "binomial")
plot(cv.lasso)

cv.lasso$lambda.min
coef(cv.lasso, cv.lasso$lambda.min)
cv.lasso$lambda.1se
coef(cv.lasso, cv.lasso$lambda.1se)


model.lasso <- glmnet(x, train_obj1$top.10, alpha = 1, family = "binomial",
                      lambda = cv.lasso$lambda.1se)

coef(model.lasso)

x.test <- model.matrix(top.10~assists + boosts + heals + teamKills + weaponsAcquired +
                    damageDealt + headshotKills + kills + killStreaks + roadKills + vehicleDestroys + 
                    killPlace + killPoints  + rankPoints + winPoints +
                    longestKill + swimDistance + rideDistance + walkDistance, data=test_obj1)

lasso.pred <- predict(model.lasso, newx = x.test)
plotRoc(lasso.pred,test_obj1$top.10)
confusionMatrix(as.factor(as.numeric(lasso.pred > 0.5)), test_obj1$top.10, positive = "1")

model.final.lasso <- glm(top.10~assists + boosts + heals + teamKills + weaponsAcquired +
                    kills + killStreaks + roadKills + vehicleDestroys + 
                    killPlace + rankPoints + winPoints +
                    longestKill + swimDistance + rideDistance + walkDistance,
                  data= train_obj1, family = binomial(link="logit"))
summary(model.final.lasso)
vif(model.final.lasso) ## Kill points and win points super correlated
```

#### Final Model for Objective 1

```{r appendix_obj1_finald, message=FALSE, include=TRUE, echo=FALSE, eval=TRUE}
#assists + boosts + heals + weaponsAcquired + kills + walkDistance
model.inf4 <- glm(top.10~assists + boosts + heals + weaponsAcquired + killPlace + walkDistance ,
                  data= train_obj1, family = binomial(link="logit"))
summary(model.inf4)
#vif(model.inf4)
#plot(model.inf4, which = 4, id.n = 3)

#scoreMod(model.inf4)
```

### Complex Logistic Regression

#### Variable Definitions  
```{r appendix_vardef, message=FALSE, include=TRUE, echo=TRUE, eval=TRUE}
#train_obj1 <- train %>% mutate(
#  DurationCut = as.factor(ifelse(matchDuration > 1600,1,0)),
#  kpCut = as.factor(ifelse(killPoints > 0,1,0)),
#  wpCut = as.factor(ifelse(winPoints > 0,1,0)),
#  rpCut = as.factor(ifelse(rankPoints > -1,1,0)),
#  boostCut = as.factor(ifelse(boosts >= 2,1,0)),
#  healCut = as.factor(ifelse(heals >= 2,1,0)),
#  walkCut = as.factor(ifelse(walkDistance >= 1250,1,0)),
#  weaponCut = as.factor(ifelse(weaponsAcquired >= 3,1,0)),
#  killCut = as.factor(ifelse(kills >= 2,1,0)),
#  assistCut = as.factor(ifelse(assists >= 2,1,0)),
#  damageCut = as.factor(ifelse(damageDealt >= 250,1,0)),
#  streakCut = as.factor(ifelse(killStreaks >= 1,1,0)),
#  killsPK = kills/(walkDistance/1000+1),
#  damageKill = kills/(damageDealt+1)
#)
```

#### Full Model List
```{r appendix_obj2_mods, message=FALSE, include=TRUE, echo=FALSE, eval=TRUE}
model.01 <- glm(top.10~boosts + assists + weaponsAcquired + killPlace + walkDistance + boostCut:killPlace ,
                  data= train_obj1, family = binomial(link="logit"))
summary(model.01)
scoreMod(model.01)

model.02 <- glm(top.10~boosts + assists + weaponsAcquired + killPlace + walkDistance + kpCut:killPlace,
                data= train_obj1, family = binomial(link="logit"))
summary(model.02)
scoreMod(model.02)

model.03 <- glm(top.10~boosts + assists + weaponsAcquired + killPlace + walkDistance + walkDistance*killPlace ,
                data= train_obj1, family = binomial(link="logit"))
summary(model.03)
scoreMod(model.03)

model.04 <- glm(top.10~boosts + assists + weaponsAcquired + killPlace + walkDistance + walkBin ,
                data= train_obj1, family = binomial(link="logit"))
summary(model.04)
scoreMod(model.04)

model.05 <- glm(top.10~boosts + assists + weaponsAcquired + killPlace + walkDistance + kpCut +  kpCut:killPlace,
                data= train_obj1, family = binomial(link="logit"))
summary(model.05)
scoreMod(model.05)

model.06 <- glm(top.10~boosts + assists + weaponsAcquired + killPlace + walkDistance + damageCut:streakCut,
                data= train_obj1, family = binomial(link="logit"))
summary(model.06)
scoreMod(model.06)

model.07 <- glm(top.10~boosts + assists + weaponsAcquired + killPlace + walkDistance + killsPK,
                data= train_obj1, family = binomial(link="logit"))
summary(model.07)
scoreMod(model.07)

model.08 <- glm(top.10~boosts + assists + weaponsAcquired + killPlace + walkDistance + damageKill + killsPK,
                data= train_obj1, family = binomial(link="logit"))
summary(model.08)
scoreMod(model.08)
```



### LDA

#### LASSO

```{r code.lda.datapull, message=FALSE, include=TRUE, echo=TRUE, eval=FALSE}
# Reduce variables to relevant continuous variables
train1<-train[,c(5:6,8:10,12:15,19,21:27,29:30)]
test1<-test[,c(5:6,8:10,12:15,19,21:27,29:30)]
# train2 <- train1[,c(1,4:19)]
# test2 <- test1[,c(1,4:19)]
train2_alt <- train1[,c(1,4:17,19)]
test2_alt <- test1[,c(1,4:17,19)]
```

```{r code.lda.lasso,  message=FALSE, include=TRUE, echo=TRUE, eval=FALSE}

# Get data in format for LASSO
x=model.matrix(top.10~.,train2_alt)[,-1]
y=as.numeric(train2_alt$top.10)

xtest<-model.matrix(top.10~.,test2_alt)[,-1]
ytest<-as.numeric(test2_alt$top.10)

grid=10^seq(10,-2, length =100)
lasso.mod=glmnet(x,y,alpha=1, lambda =grid)

set.seed(23) #removes kills roadKills vehicleDestroys
lda.cv.out=cv.glmnet(x,y,alpha=1,family="binomial") #alpha=1 performs LASSO
# plot(lda.cv.out)

# simplest model
lda.lasso.model.coef<-coef(cv.out, cv.out$lambda.1se)
# lda.lasso.model.coef
```

```{r code.lda.lasso.output , echo=TRUE, eval=TRUE, fig.cap="Lasso Coefficients for LDA"}
lda.lasso.model.coef
```


```{r code.lda.train_final,  message=FALSE, include=TRUE, echo=TRUE, eval=FALSE}

# Removing the kills, roadKills, and vehicleDestroys as shown above
# Also removing rankPoints because 
# (a) the majority of the players in these data aren't ranked and 
# (b) the Kaggle descrption says "This ranking is inconsistent and is being deprecated in the API's next version"
train_final <- train2_alt[,c(-4,-8,-10,-13)]

```
#### Assumption Checking

```{r code.lda.assumptions.1,  message=FALSE, include=TRUE, echo=TRUE, eval=FALSE}

train_final_no <- train_final[which(train_final$top.10==0),]
train_final_yes <- train_final[which(train_final$top.10==1),]

# nrow(train_final)
# nrow(train_final_no)
# nrow(train_final_yes)

max_cols<-ncol(train_final)
no_matrix<-as.matrix(train_final_no[,-max_cols])
yes_matrix<-as.matrix(train_final_yes[,-max_cols])

no_hist<-multi.hist(no_matrix)
yes_hist<-multi.hist(yes_matrix)
# par(mfrow=c(1,1))

```

```{r code.lda.no.hist , echo=TRUE, eval=TRUE, fig.cap="Histograms for Bottom 90 of Training Dataset"}
no_hist
```

```{r code.lda.yes.hist , echo=TRUE, eval=TRUE, fig.cap="Histograms for Top 90 of Training Dataset"}
yes_hist
```

```{r code.lda.assumptions.2,  message=FALSE, include=TRUE, echo=TRUE, eval=FALSE}

#http://www.sthda.com/english/wiki/ggplot2-quick-correlation-matrix-heatmap-r-software-and-data-visualization
get_upper_tri <- function(cormat){
  cormat[lower.tri(cormat)]<- NA
  return(cormat)
}

custom_corr_plot <- function(cormat){
  
  upper_tri <- get_upper_tri(cormat)
  # Melt the correlation matrix
  melted_cormat <- melt(upper_tri, na.rm = TRUE)
  # Create a ggheatmap
  ggheatmap <- ggplot(melted_cormat, aes(Var2, Var1, fill = value))+
    geom_tile(color = "white")+
    scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                         midpoint = 0, limit = c(-1,1), space = "Lab", 
                         name="Pearson\nCorrelation") +
    theme_minimal()+ # minimal theme
    theme(axis.text.x = element_text(angle = 45, vjust = 1, 
                                     size = 12, hjust = 1))+
    coord_fixed()
  
  
  p<-ggheatmap + 
    geom_text(aes(Var2, Var1, label = value), color = "black", size = 4) +
    theme(
      axis.title.x = element_blank(),
      axis.title.y = element_blank(),
      panel.grid.major = element_blank(),
      panel.border = element_blank(),
      panel.background = element_blank(),
      axis.ticks = element_blank(),
      legend.justification = c(1, 0),
      legend.position = c(0.6, 0.7),
      legend.direction = "horizontal")+
    guides(fill = guide_colorbar(barwidth = 7, barheight = 1,
                                 title.position = "top", title.hjust = 0.5))
  
  return(p)
}

cormat <- round(cor(no_matrix),2)
lda.no <- custom_corr_plot(cormat)

cormat <- round(cor(yes_matrix),2)
lda.yes <- custom_corr_plot(cormat)

```

```{r code.lda.no , echo=TRUE, eval=TRUE, fig.cap="Correlation Matrix for Bottom 90 of Training Dataset"}
lda.no
```

```{r code.lda.yes , echo=TRUE, eval=TRUE, fig.cap="Correlation Matrix for Top 90 of Training Dataset"}
lda.yes
```

#### LDA Results

```{r code.lda.model,  message=FALSE, include=TRUE, echo=TRUE, eval=TRUE}

# Run LDA
lda <- lda(top.10 ~ . , data=train_final, prior=c(.9,.1))

lda

predict <- predict(lda,newdata=test)

test_final <- test
test_final$predcited_place <- as.vector(predict$class)
test_final$top.10<-as.factor(test_final$top.10)
test_final$predcited_place<-as.factor(test_final$predcited_place)

xtab<-table(test_final$predcited_place,test_final$top.10)
confusionMatrix(xtab, positive="1")

predict.posteriors <- as.data.frame(predict$posterior)

# Evaluate the model
pred <- prediction(predict.posteriors[,2], test$top.10)
roc.perf = performance(pred, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred, measure = "auc")
auc.train <- auc.train@y.values


# Plot
plot(roc.perf, main="ROC Curve - LDA")
abline(a=0, b= 1)
text(x = .25, y = .65 ,paste("AUC = ", round(auc.train[[1]],3), sep = ""))


```

#### QDA Results

```{r code.qda.model,  message=FALSE, include=TRUE, echo=TRUE, eval=TRUE}

qda <- qda(top.10 ~ . , data=train_final, prior=c(.9,.1))

qda

predict.qda <- predict(qda,newdata=test)

# test_final <- test
test_final$predcited_place.qda <- as.vector(predict$class)
# test_final$top.10<-as.factor(test_final$top.10)
test_final$predcited_place.qda<-as.factor(test_final$predcited_place.qda)
# test_final$predcited_place<-as.factor(test_final$predcited_place)
# str(test_final)

xtab.qda<-table(test_final$predcited_place.qda,test_final$top.10)
confusionMatrix(xtab.qda, positive="1")

predict.posteriors.qda <- as.data.frame(predict.qda$posterior)

# Evaluate the model
pred.qda <- prediction(predict.posteriors.qda[,2], test$top.10)
roc.perf.qda = performance(pred.qda, measure = "tpr", x.measure = "fpr")
auc.train.qda <- performance(pred.qda, measure = "auc")
auc.train.qda <- auc.train.qda@y.values


# Plot
plot(roc.perf.qda, main="ROC Curve - QDA")
abline(a=0, b= 1)
text(x = .25, y = .65 ,paste("AUC = ", round(auc.train[[1]],3), sep = ""))


```


### Random Forest

#### Model

```{r rf.model.appendix, eval=TRUE}

# results='hide', message=FALSE, include=FALSE, echo=FALSE
rf_cols_to_remove = c("Id", "groupId", "matchId", "matchType", "DBNOs", "revives", "winPlacePerc")

train.rf <- train %>%
  dplyr::select(-rf_cols_to_remove) %>%
  mutate(kills  = kills * 100 / numGroups) %>% # Normalized Kills
  mutate(matchDuration = as.factor(ifelse(matchDuration < mean(matchDuration), "Low", "High"))) %>%
  mutate(top.10 = factor(top.10, labels = c("No", "Yes"))) 


test.rf <- test %>%
  dplyr::select(-rf_cols_to_remove) %>%
  mutate(kills  = kills * 100 / numGroups) %>% # Normalized Kills
  mutate(matchDuration = as.factor(ifelse(matchDuration < mean(matchDuration), "Low", "High"))) %>%
  mutate(top.10 = factor(top.10, labels = c("No", "Yes"))) 

set.seed(1234)
model.rf <- randomForest(as.factor(top.10) ~ ., 
                               data = train.rf, 
                               ntree = 500, 
                               mtry = 12, 
                               cutoff = c(0.4,1-0.40))


model.rf

```

#### Tuning Plot

```{r rf.model.plot.appendix, eval=TRUE}

plot(model.rf)

```

#### Variable Importance

```{r rf.variable.importance.appendix, eval=TRUE}

# varImp(model.rf)

rf.imp.variables <-data.frame(varImp(model.rf))

# rf.imp.variables[order(rf.imp.variables$Overall,decreasing = T),]

kable(data.frame('Column Name' = rownames(rf.imp.variables), 
                 'Overall Score'= rf.imp.variables$Overall),
      "latex", booktabs = T)  %>%
  kable_styling(position = "center")

```


#### Predict

```{r rf.predict.appendix, eval=FALSE}


prd.rf.train <- predict(model.rf, train.rf)
prd.rf.test <- predict(model.rf, test.rf)

```

#### Confusion Matrix

```{r rf.confusion.matrix.appendix, eval=TRUE}
confusionMatrix(data=prd.rf.test,
                reference=test.rf$top.10, "Yes")
```

#### ROC Curve

```{r rf.area.under.curve.appendix, eval=TRUE, fig.align = "center"}
prd.rf.test <- predict(model.rf, test.rf, type = "prob")

predictions <- as.vector(prd.rf.test[,2])

pred <- prediction(predictions, test$top.10)

perf_AUC <- performance(pred,"auc") #Calculate the AUC value
AUC=perf_AUC@y.values[[1]]

perf_ROC=performance(pred,"tpr","fpr") #plot the actual ROC curve
plot(perf_ROC, main="Random Forest")
text(0.5,0.5,paste("AUC = ",format(AUC, digits=3, scientific=FALSE)))
abline(a=0, b= 1) #Ref line indicating poor performance


```


# References