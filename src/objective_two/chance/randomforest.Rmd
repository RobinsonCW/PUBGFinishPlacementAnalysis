---
title: "Random Forest"
author: "Chance Robinson"
date: "11/30/2019"
output: 
  github_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Exploratory Data Analysis

## Library Imports


```{r library-imports, quietely=TRUE, warn.conflicts=FALSE}
library(tidyverse)
# Random Forest
library(randomForest)  
library(e1071)  
# downSample
library(caret)
# ROC Curves
library(ROCR)
library(pROC)
```



## Load the CSV Data

```{r load-data}
data <- read.csv("../../../data/pubg_solo_game_types.csv", stringsAsFactors=FALSE)

```


```{r head-data}
head(data)
```

## Remove Missing Values

```{r data-missing}
# remove the row with no winPlacePerc   
data <- data[!data$Id == 'f70c74418bb064',]


# summary(data$rankPoints)
# 
# data$rankPoints <- cut(data$rankPoints, breaks=5)
# 
# str(data$rankPoints)
# (data[data$winPoints == 0,])
# 
# (data[data$killPoints == 0,])
# 
# (data[data$rankPoints != -1,])


# data <- data %>%
#   filter(rankPoints != -1 & winPoints == 0) %>%
#   mutate(winPoints = 0)
# 
# data <- data %>%
#   filter(rankPoints != -1 & killPoints == 0) %>%
#   mutate(killPoints = 0)
# 
# 
# outliers <- data[   data$walkDistance > mean(data$walkDistance) + (sd(data$walkDistance) * 3), ]
# 
# outliers
# 
# outliers
# dim(data)

```



## Specify Model Columns of Interest

```{r data-significant-columns}
cols_to_keep = c("walkDistance", "killPlace", "boosts", "weaponsAcquired", "damageDealt", "heals", "kills", "top.10")

cols_to_remove = c("Id", "groupId", "matchId", "matchType", "DBNOs", "revives", "winPlacePerc")

head(data[cols_to_keep])


```


## Prepare Dataframe

```{r data-preparation}
data.mod <- data %>%
  select(-cols_to_remove) %>%
  mutate(top.10 = factor(top.10, labels = c("No", "Yes"))) 

summary(data.mod)

# str(data.mod)

```


## Random Forrest


## Train / Test Split

```{r rf-train-test-split}

set.seed(1234)

sample.data <- sample_frac(data.mod, 1)

# sample.data <- downSample(sample.data, sample.data$top.10, list = FALSE)
# sample.data$Class <- NULL

# head(sample.data)

split.perc = .70

train.indices = sample(1:dim(sample.data)[1],round(split.perc * dim(sample.data)[1]))

train = sample.data[train.indices,]
test = sample.data[-train.indices,]

train <- downSample(train, train$top.10, list = FALSE)
train$Class <- NULL


model.rf.train <- randomForest(top.10 ~ ., data = train, ntree = 275, mtry = 8, cutoff = c(0.36,1-0.36))

?randomForest

print(model.rf.train)


p1 <- predict(model.rf.train, train)
p2 <- predict(model.rf.train, test)

head(p1)
plot(model.rf.train) 
varImp(model.rf.train)


```



## Random Forest Performance

### Train
```{r rf-performance-train}

confusionMatrix(data=p1,  
                reference=train$top.10, "Yes")

```

### Test
```{r rf-performance-test}


confusionMatrix(data=p2,  
                reference=test$top.10, "Yes")

```

```{r rf-tune}

# head(test)
tuneRF(test[,-23], test[,23],
       stepFactor = 0.5,
       ntreeTry = 275,
       trace = TRUE,
       improve = 0.5
)

```



### Area Under the Curve

```{r rf-auc-plot-test}

# ?pROC

auc <- roc(as.integer(test$top.10), as.integer(p2))
# print(auc)

# plot(auc, ylim=c(0,1), print.thres=TRUE, main=paste('AUC of Test Set:', round(auc$auc[[1]],2)))
# abline(h=1,col='green',lwd=2)
# abline(h=0,col='red',lwd=2)

g <- ggroc(auc, alpha = 0.5, colour = "red", linetype = 2, size = 2) +
  theme_minimal() + 
  ggtitle(paste('AUC of Test Set:', round(auc$auc[[1]],2))) + 
  geom_segment(aes(x = 0, xend = 1, y = 0, yend = 1), color="darkgrey", linetype="dashed")
  

plot(g)



```

```{r pre-train-test-split}

# train <- read.csv("../../../data/pubg_solo_game_types_train_downsampled.csv", stringsAsFactors=FALSE)
# 
# test <- read.csv("../../../data/pubg_solo_game_types_test_full.csv", stringsAsFactors=FALSE)
# 
# 
# train <- train %>%
#   select(-cols_to_remove) %>%
#   mutate(top.10 = factor(top.10, labels = c("No", "Yes")))
# 
# 
# test <- test %>%
#   select(-cols_to_remove) %>%
#   mutate(top.10 = factor(top.10, labels = c("No", "Yes")))
# 
# 
# model.rf.train <- randomForest(as.factor(top.10) ~ ., data = train, ntree = 275, mtry = 8, cutoff = c(0.36,1-0.36))
# 
# print(model.rf.train)
# 
# 
# p1 <- predict(model.rf.train, train)
# p2 <- predict(model.rf.train, test)
# 
# 
# print(model.rf.train)
# plot(model.rf.train)
# varImp(model.rf.train)
# 
# 
# confusionMatrix(data=p1,
#                 reference=train$top.10, "Yes")
# 
# 
# confusionMatrix(data=p2,
#                 reference=test$top.10, "Yes")

```

